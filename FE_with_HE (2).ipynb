{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m-UMDkdjuGMv",
        "outputId": "42131e0e-4986-4328-c671-8af9537fb6eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Device: cpu\n",
            "GPU: CPU\n",
            "Bootstrap completed successfully.\n",
            "Ready for Phase 5 (Model Definition).\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# BOOTSTRAP CELL — RUN THIS FIRST AFTER EVERY RUNTIME RESET\n",
        "\n",
        "\n",
        "import os\n",
        "import random\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    precision_score,\n",
        "    recall_score,\n",
        "    f1_score,\n",
        "    confusion_matrix\n",
        ")\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "MODEL_VERSION = \"FL with HE\"\n",
        "\n",
        "BASE_DIR = \"/content/drive/MyDrive/FYP_FL_IDS\"\n",
        "\n",
        "RAW_DATA_DIR = os.path.join(BASE_DIR, \"data/raw\")\n",
        "PROCESSED_DATA_DIR = os.path.join(BASE_DIR, \"data/processed\")\n",
        "\n",
        "MODELS_DIR = os.path.join(BASE_DIR, \"models\")\n",
        "FL_DIR = os.path.join(BASE_DIR, \"fl\")\n",
        "RESULTS_DIR = os.path.join(BASE_DIR, \"results\")\n",
        "\n",
        "os.makedirs(MODELS_DIR, exist_ok=True)\n",
        "os.makedirs(FL_DIR, exist_ok=True)\n",
        "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
        "\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Device:\", DEVICE)\n",
        "print(\"GPU:\", torch.cuda.get_device_name(0) if DEVICE == \"cuda\" else \"CPU\")\n",
        "\n",
        "CONFIG = {\n",
        "    \"SEED\": SEED,\n",
        "    \"NUM_CLIENTS\": 3,\n",
        "    \"BATCH_SIZE\": 128,\n",
        "    \"LOCAL_EPOCHS\": 1,\n",
        "    \"ROUNDS\": 25,\n",
        "    \"LEARNING_RATE\": 1e-3,\n",
        "    \"SEQUENCE_LENGTH\": 10,\n",
        "    \"NUM_CLASSES\": 1,\n",
        "    \"DEVICE\": DEVICE,\n",
        "}\n",
        "\n",
        "\n",
        "POS_WEIGHT = torch.tensor([5.0], device=DEVICE)\n",
        "CRITERION = nn.BCEWithLogitsLoss(pos_weight=POS_WEIGHT)\n",
        "\n",
        "\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(\"Bootstrap completed successfully.\")\n",
        "print(\"Ready for Phase 5 (Model Definition).\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RL58ev2lZ7oO",
        "outputId": "8bbf1c9d-5c52-4fa8-8623-03b9bc20f430"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'SEED': 42,\n",
              " 'NUM_CLIENTS': 3,\n",
              " 'BATCH_SIZE': 64,\n",
              " 'LOCAL_EPOCHS': 1,\n",
              " 'ROUNDS': 12,\n",
              " 'LEARNING_RATE': 0.001,\n",
              " 'SEQUENCE_LENGTH': 10,\n",
              " 'DEVICE': 'cuda'}"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ],
      "source": [
        "CONFIG = {\n",
        "    \"SEED\": 42,\n",
        "    \"NUM_CLIENTS\": 3,\n",
        "    \"BATCH_SIZE\": 64,\n",
        "    \"LOCAL_EPOCHS\": 1,\n",
        "    \"ROUNDS\": 12,\n",
        "    \"LEARNING_RATE\": 1e-3,\n",
        "    \"SEQUENCE_LENGTH\": 10,\n",
        "    \"DEVICE\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
        "}\n",
        "\n",
        "CONFIG\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IJoLncAdYX75",
        "outputId": "63443725-4ae5-4519-ab15-903a0d01b35d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MYIGyD3lYdjb",
        "outputId": "83fdaa64-92ec-403e-be60-2d547dddcbf6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Project directory structure created.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "BASE_DIR = \"/content/drive/MyDrive/FYP_FL_IDS\"\n",
        "\n",
        "folders = [\n",
        "    \"data/raw\",\n",
        "    \"data/processed\",\n",
        "    \"models\",\n",
        "    \"fl\",\n",
        "    \"utils\",\n",
        "    \"results/logs\",\n",
        "    \"results/plots\",\n",
        "    \"notebooks\"\n",
        "]\n",
        "\n",
        "for folder in folders:\n",
        "    path = os.path.join(BASE_DIR, folder)\n",
        "    os.makedirs(path, exist_ok=True)\n",
        "\n",
        "print(\"Project directory structure created.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cN5wDGRSZmO2",
        "outputId": "d55dfb9a-31e6-422f-eeb5-b649064ca87c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch version: 2.9.0+cu126\n",
            "CUDA available: True\n",
            "GPU: Tesla T4\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "print(\"PyTorch version:\", torch.__version__)\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(\"GPU:\", torch.cuda.get_device_name(0))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sPN5KRV3ZrKH"
      },
      "outputs": [],
      "source": [
        "!pip install -q \\\n",
        "    numpy \\\n",
        "    pandas \\\n",
        "    scikit-learn \\\n",
        "    imbalanced-learn \\\n",
        "    matplotlib \\\n",
        "    seaborn \\\n",
        "    tqdm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rM9waC70OcgS",
        "outputId": "c1f904e9-2255-47a1-8b66-f413c7cac6f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/4.8 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/4.8 MB\u001b[0m \u001b[31m70.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m67.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# Install TenSEAL for Homomorphic Encryption\n",
        "!pip install -q tenseal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ppRsMa2MZxjr",
        "outputId": "b8319430-62ad-4f13-a039-4b16c61bc49c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NumPy: 2.0.2\n",
            "Pandas: 2.2.2\n",
            "Scikit-learn: 1.6.1\n",
            "Torch: 2.9.0+cu126\n",
            "Matplotlib: 3.10.0\n",
            "Seaborn: 0.13.2\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sklearn\n",
        "import torch\n",
        "import matplotlib\n",
        "import seaborn\n",
        "\n",
        "print(\"NumPy:\", np.__version__)\n",
        "print(\"Pandas:\", pd.__version__)\n",
        "print(\"Scikit-learn:\", sklearn.__version__)\n",
        "print(\"Torch:\", torch.__version__)\n",
        "print(\"Matplotlib:\", matplotlib.__version__)\n",
        "print(\"Seaborn:\", seaborn.__version__)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LLuoEDJdZ3Cy",
        "outputId": "e02d0563-1bc3-456c-fb56-af1efa4e876d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random seeds fixed to: 42\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "import torch\n",
        "\n",
        "SEED = 42\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "print(\"Random seeds fixed to:\", SEED)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rWetBwvDaBMI",
        "outputId": "59737413-f4a0-442f-c886-70bb0224a776"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files found:\n",
            " - Wednesday-workingHours.pcap_ISCX.csv\n",
            " - Tuesday-WorkingHours.pcap_ISCX.csv\n",
            " - Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX.csv\n",
            " - Friday-WorkingHours-Morning.pcap_ISCX.csv\n",
            " - Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv\n",
            " - Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "raw_data_path = os.path.join(BASE_DIR, \"data/raw\")\n",
        "files = os.listdir(raw_data_path)\n",
        "\n",
        "print(\"Files found:\")\n",
        "for f in files:\n",
        "    print(\" -\", f)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QYa4QCpMr19F",
        "outputId": "145e9d9f-c96f-43f0-92ba-7b6c471e66ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading Wednesday-workingHours.pcap_ISCX.csv ...\n",
            "Loading Tuesday-WorkingHours.pcap_ISCX.csv ...\n",
            "Loading Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX.csv ...\n",
            "Loading Friday-WorkingHours-Morning.pcap_ISCX.csv ...\n",
            "Loading Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv ...\n",
            "Loading Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv ...\n",
            "\n",
            "All files loaded successfully.\n"
          ]
        }
      ],
      "source": [
        "dataframes = []\n",
        "\n",
        "for file in files:\n",
        "    file_path = os.path.join(RAW_DATA_DIR, file)\n",
        "    print(f\"Loading {file} ...\")\n",
        "\n",
        "    df = pd.read_csv(file_path, low_memory=False)\n",
        "    df[\"source_file\"] = file   # keep traceability\n",
        "\n",
        "    dataframes.append(df)\n",
        "\n",
        "print(\"\\nAll files loaded successfully.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3PxpMUYtuEu8",
        "outputId": "31c7bb9f-09fe-42d0-8732-1872005fdb22"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Combined dataset shape: (2012223, 80)\n",
            "Columns in dataset:\n",
            "\n",
            " Destination Port\n",
            " Flow Duration\n",
            " Total Fwd Packets\n",
            " Total Backward Packets\n",
            "Total Length of Fwd Packets\n",
            " Total Length of Bwd Packets\n",
            " Fwd Packet Length Max\n",
            " Fwd Packet Length Min\n",
            " Fwd Packet Length Mean\n",
            " Fwd Packet Length Std\n",
            "Bwd Packet Length Max\n",
            " Bwd Packet Length Min\n",
            " Bwd Packet Length Mean\n",
            " Bwd Packet Length Std\n",
            "Flow Bytes/s\n",
            " Flow Packets/s\n",
            " Flow IAT Mean\n",
            " Flow IAT Std\n",
            " Flow IAT Max\n",
            " Flow IAT Min\n",
            "Fwd IAT Total\n",
            " Fwd IAT Mean\n",
            " Fwd IAT Std\n",
            " Fwd IAT Max\n",
            " Fwd IAT Min\n",
            "Bwd IAT Total\n",
            " Bwd IAT Mean\n",
            " Bwd IAT Std\n",
            " Bwd IAT Max\n",
            " Bwd IAT Min\n",
            "Fwd PSH Flags\n",
            " Bwd PSH Flags\n",
            " Fwd URG Flags\n",
            " Bwd URG Flags\n",
            " Fwd Header Length\n",
            " Bwd Header Length\n",
            "Fwd Packets/s\n",
            " Bwd Packets/s\n",
            " Min Packet Length\n",
            " Max Packet Length\n",
            " Packet Length Mean\n",
            " Packet Length Std\n",
            " Packet Length Variance\n",
            "FIN Flag Count\n",
            " SYN Flag Count\n",
            " RST Flag Count\n",
            " PSH Flag Count\n",
            " ACK Flag Count\n",
            " URG Flag Count\n",
            " CWE Flag Count\n",
            " ECE Flag Count\n",
            " Down/Up Ratio\n",
            " Average Packet Size\n",
            " Avg Fwd Segment Size\n",
            " Avg Bwd Segment Size\n",
            " Fwd Header Length.1\n",
            "Fwd Avg Bytes/Bulk\n",
            " Fwd Avg Packets/Bulk\n",
            " Fwd Avg Bulk Rate\n",
            " Bwd Avg Bytes/Bulk\n",
            " Bwd Avg Packets/Bulk\n",
            "Bwd Avg Bulk Rate\n",
            "Subflow Fwd Packets\n",
            " Subflow Fwd Bytes\n",
            " Subflow Bwd Packets\n",
            " Subflow Bwd Bytes\n",
            "Init_Win_bytes_forward\n",
            " Init_Win_bytes_backward\n",
            " act_data_pkt_fwd\n",
            " min_seg_size_forward\n",
            "Active Mean\n",
            " Active Std\n",
            " Active Max\n",
            " Active Min\n",
            "Idle Mean\n",
            " Idle Std\n",
            " Idle Max\n",
            " Idle Min\n",
            " Label\n",
            "source_file\n"
          ]
        }
      ],
      "source": [
        "df_all = pd.concat(dataframes, axis=0, ignore_index=True)\n",
        "\n",
        "print(\"Combined dataset shape:\", df_all.shape)\n",
        "\n",
        "print(\"Columns in dataset:\\n\")\n",
        "for col in df_all.columns:\n",
        "    print(col)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xbAAWj-5vVPf",
        "outputId": "4a227284-a714-4aaf-f678-62f5c20ed0e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dropped columns: []\n",
            "Remaining shape: (2012223, 80)\n",
            "Total NaN values in dataset: 4446\n",
            "NaN values after imputation: 0\n"
          ]
        }
      ],
      "source": [
        "DROP_COLUMNS = [\n",
        "    \"Flow ID\",\n",
        "    \"Source IP\",\n",
        "    \"Destination IP\",\n",
        "    \"Timestamp\"\n",
        "]\n",
        "\n",
        "existing_drop_cols = [c for c in DROP_COLUMNS if c in df_all.columns]\n",
        "\n",
        "df_all.drop(columns=existing_drop_cols, inplace=True)\n",
        "\n",
        "print(\"Dropped columns:\", existing_drop_cols)\n",
        "print(\"Remaining shape:\", df_all.shape)\n",
        "\n",
        "df_all.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "nan_count = df_all.isna().sum().sum()\n",
        "print(\"Total NaN values in dataset:\", nan_count)\n",
        "\n",
        "numeric_cols = df_all.select_dtypes(include=[np.number]).columns\n",
        "\n",
        "df_all[numeric_cols] = df_all[numeric_cols].fillna(\n",
        "    df_all[numeric_cols].median()\n",
        ")\n",
        "\n",
        "print(\"NaN values after imputation:\", df_all.isna().sum().sum())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i3VeSpubjYIO",
        "outputId": "83f492ec-3378-4925-fcff-1b1986dcfe98"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label\n",
            "BENIGN                        1454613\n",
            "DoS Hulk                       231073\n",
            "PortScan                       158930\n",
            "DDoS                           128027\n",
            "DoS GoldenEye                   10293\n",
            "FTP-Patator                      7938\n",
            "SSH-Patator                      5897\n",
            "DoS slowloris                    5796\n",
            "DoS Slowhttptest                 5499\n",
            "Bot                              1966\n",
            "Web Attack � Brute Force         1507\n",
            "Web Attack � XSS                  652\n",
            "Web Attack � Sql Injection         21\n",
            "Heartbleed                         11\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "df_all.columns = df_all.columns.str.strip()\n",
        "print(df_all[\"Label\"].value_counts())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FNuw56XwjmHp",
        "outputId": "f376cbfa-a8e8-4740-a8aa-a06ea67d4531"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['BENIGN' 'DoS slowloris' 'DoS Slowhttptest' 'DoS Hulk' 'DoS GoldenEye'\n",
            " 'Heartbleed' 'FTP-Patator' 'SSH-Patator' 'Web Attack � Brute Force'\n",
            " 'Web Attack � XSS' 'Web Attack � Sql Injection' 'Bot' 'PortScan' 'DDoS']\n",
            "['benign' 'dos slowloris' 'dos slowhttptest' 'dos hulk' 'dos goldeneye'\n",
            " 'heartbleed' 'ftp-patator' 'ssh-patator' 'web attack � brute force'\n",
            " 'web attack � xss' 'web attack � sql injection' 'bot' 'portscan' 'ddos']\n",
            "['benign' 'dos slowloris' 'dos slowhttptest' 'dos hulk' 'dos goldeneye'\n",
            " 'heartbleed' 'ftp patator' 'ssh patator' 'web attack brute force'\n",
            " 'web attack xss' 'web attack sql injection' 'bot' 'portscan' 'ddos']\n",
            "Label\n",
            "0    1454613\n",
            "1     557610\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "print(df_all[\"Label\"].unique())\n",
        "df_all[\"Label\"] = df_all[\"Label\"].astype(str).str.strip().str.lower()\n",
        "print(df_all[\"Label\"].unique())\n",
        "\n",
        "import re\n",
        "\n",
        "def clean_label(label):\n",
        "    label = label.lower()\n",
        "    label = label.strip()\n",
        "    label = re.sub(r'[^a-z0-9\\s]', ' ', label)  # remove special chars\n",
        "    label = re.sub(r'\\s+', ' ', label)          # normalize spaces\n",
        "    return label\n",
        "\n",
        "df_all[\"Label\"] = df_all[\"Label\"].astype(str).apply(clean_label)\n",
        "print(df_all[\"Label\"].unique())\n",
        "df_all[\"Label\"] = df_all[\"Label\"].apply(\n",
        "    lambda x: 0 if x == \"benign\" else 1\n",
        ")\n",
        "print(df_all[\"Label\"].value_counts())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8lUR-K1nh6Ri",
        "outputId": "1d90b5bf-516d-482b-a2a3-8daa6923dcc5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Phase 1 cleaned dataset saved at: /content/drive/MyDrive/FYP_FL_IDS/data/processed/cic_ids2017_clean_phase1.csv\n"
          ]
        }
      ],
      "source": [
        "clean_path = os.path.join(PROCESSED_DATA_DIR, \"cic_ids2017_clean_phase1.csv\")\n",
        "df_all.to_csv(clean_path, index=False)\n",
        "\n",
        "print(\"Phase 1 cleaned dataset saved at:\", clean_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Xfqw03hmGic",
        "outputId": "9652f4e3-9e35-43de-ed8f-afbd7653f7bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset loaded.\n",
            "Shape: (2012223, 80)\n",
            "Feature matrix shape: (2012223, 79)\n",
            "Label vector shape: (2012223,)\n",
            "Categorical columns: ['source_file']\n",
            "Dropped column: source_file\n",
            "Updated feature shape: (2012223, 78)\n",
            "Converted features to float32.\n",
            "Categorical columns: []\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "clean_path = os.path.join(PROCESSED_DATA_DIR, \"cic_ids2017_clean_phase1.csv\")\n",
        "\n",
        "df = pd.read_csv(clean_path)\n",
        "\n",
        "print(\"Dataset loaded.\")\n",
        "print(\"Shape:\", df.shape)\n",
        "\n",
        "X = df.drop(columns=[\"Label\"])\n",
        "y = df[\"Label\"]\n",
        "\n",
        "print(\"Feature matrix shape:\", X.shape)\n",
        "print(\"Label vector shape:\", y.shape)\n",
        "categorical_cols = X.select_dtypes(include=[\"object\"]).columns.tolist()\n",
        "\n",
        "print(\"Categorical columns:\", categorical_cols)\n",
        "if \"source_file\" in X.columns:\n",
        "    X = X.drop(columns=[\"source_file\"])\n",
        "    print(\"Dropped column: source_file\")\n",
        "\n",
        "print(\"Updated feature shape:\", X.shape)\n",
        "X = X.astype(\"float32\")\n",
        "\n",
        "print(\"Converted features to float32.\")\n",
        "\n",
        "categorical_cols = X.select_dtypes(include=[\"object\"]).columns.tolist()\n",
        "\n",
        "print(\"Categorical columns:\", categorical_cols)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mNvIgqC2vanN",
        "outputId": "5a463f46-42d9-4871-89dd-b350333f79db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature scaling completed.\n",
            "Mean (first 5 features): [-1.5188851e-08 -3.7338943e-08  6.9669276e-10  1.0995423e-09\n",
            "  4.4076480e-09]\n",
            "Std (first 5 features): [0.9999998 0.9999999 0.9999998 1.0000001 1.0000002]\n",
            "Scaled features and labels saved.\n"
          ]
        }
      ],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "print(\"Feature scaling completed.\")\n",
        "print(\"Mean (first 5 features):\", X_scaled.mean(axis=0)[:5])\n",
        "print(\"Std (first 5 features):\", X_scaled.std(axis=0)[:5])\n",
        "\n",
        "\n",
        "np.save(os.path.join(PROCESSED_DATA_DIR, \"X_scaled.npy\"), X_scaled)\n",
        "np.save(os.path.join(PROCESSED_DATA_DIR, \"y.npy\"), y.values)\n",
        "\n",
        "print(\"Scaled features and labels saved.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "drXu-EV1wKH6",
        "outputId": "62680ac8-1b25-4afd-c671-46ab8c96ed7c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scaler saved at: /content/drive/MyDrive/FYP_FL_IDS/data/processed/standard_scaler.pkl\n",
            "Phase 2 completed successfully.\n"
          ]
        }
      ],
      "source": [
        "import joblib\n",
        "\n",
        "scaler_path = os.path.join(PROCESSED_DATA_DIR, \"standard_scaler.pkl\")\n",
        "joblib.dump(scaler, scaler_path)\n",
        "\n",
        "print(\"Scaler saved at:\", scaler_path)\n",
        "\n",
        "assert X_scaled.shape[0] == y.shape[0], \"Mismatch in X and y sizes!\"\n",
        "assert not np.isnan(X_scaled).any(), \"NaN values found!\"\n",
        "assert not np.isinf(X_scaled).any(), \"Infinity values found!\"\n",
        "\n",
        "print(\"Phase 2 completed successfully.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TuNXOqYf0CD4",
        "outputId": "3f2364cd-a56a-4156-b555-536a7688aa3e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded X_scaled (memmap): (2012223, 78)\n",
            "Loaded y (memmap): (2012223,)\n",
            "Sequence length: 10\n",
            "Total rows: 2012223\n",
            "Chunk size: 200000\n",
            "Features: 78\n"
          ]
        }
      ],
      "source": [
        "X_path = os.path.join(PROCESSED_DATA_DIR, \"X_scaled.npy\")\n",
        "y_path = os.path.join(PROCESSED_DATA_DIR, \"y.npy\")\n",
        "\n",
        "X_scaled = np.load(X_path, mmap_mode=\"r\")\n",
        "y = np.load(y_path, mmap_mode=\"r\")\n",
        "\n",
        "print(\"Loaded X_scaled (memmap):\", X_scaled.shape)\n",
        "print(\"Loaded y (memmap):\", y.shape)\n",
        "\n",
        "SEQ_LEN = CONFIG[\"SEQUENCE_LENGTH\"]\n",
        "print(\"Sequence length:\", SEQ_LEN)\n",
        "\n",
        "CHUNK_SIZE = 200_000\n",
        "\n",
        "TOTAL_ROWS = X_scaled.shape[0]\n",
        "NUM_FEATURES = X_scaled.shape[1]\n",
        "\n",
        "print(\"Total rows:\", TOTAL_ROWS)\n",
        "print(\"Chunk size:\", CHUNK_SIZE)\n",
        "print(\"Features:\", NUM_FEATURES)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mpuaqbmu0DLv"
      },
      "outputs": [],
      "source": [
        "def process_chunk1(X, y, start_idx, end_idx, seq_len):\n",
        "    X_seq = []\n",
        "    y_seq = []\n",
        "\n",
        "    for i in range(start_idx, end_idx - seq_len + 1):\n",
        "        X_seq.append(X[i:i + seq_len])\n",
        "        y_seq.append(y[i + seq_len - 1])\n",
        "\n",
        "    return np.array(X_seq, dtype=np.float32), np.array(y_seq, dtype=np.int64)\n",
        "\n",
        "def process_chunk(X, y, start_idx, end_idx, seq_len, stride=5):\n",
        "    X_seq = []\n",
        "    y_seq = []\n",
        "\n",
        "    for i in range(start_idx, end_idx - seq_len + 1, stride):\n",
        "        X_seq.append(X[i:i + seq_len])\n",
        "        y_seq.append(y[i + seq_len - 1])\n",
        "\n",
        "    return np.array(X_seq, dtype=np.float32), np.array(y_seq, dtype=np.int64)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nQOhbxwDE8sh",
        "outputId": "96605a77-bf43-4a5d-e495-9abc3779f7b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sequence_chunks cleared.\n"
          ]
        }
      ],
      "source": [
        "import shutil\n",
        "\n",
        "output_dir = os.path.join(PROCESSED_DATA_DIR, \"sequence_chunks\")\n",
        "\n",
        "if os.path.exists(output_dir):\n",
        "    shutil.rmtree(output_dir)\n",
        "\n",
        "os.makedirs(output_dir)\n",
        "print(\"sequence_chunks cleared.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RKcXVO150S6j",
        "outputId": "d08a234e-ae81-41e4-ee0e-863d19a939eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Processing chunk 0 | rows 0 to 200009\n",
            "Saved shapes: (40000, 10, 78) (40000,)\n",
            "\n",
            "Processing chunk 1 | rows 200000 to 400009\n",
            "Saved shapes: (40000, 10, 78) (40000,)\n",
            "\n",
            "Processing chunk 2 | rows 400000 to 600009\n",
            "Saved shapes: (40000, 10, 78) (40000,)\n",
            "\n",
            "Processing chunk 3 | rows 600000 to 800009\n",
            "Saved shapes: (40000, 10, 78) (40000,)\n",
            "\n",
            "Processing chunk 4 | rows 800000 to 1000009\n",
            "Saved shapes: (40000, 10, 78) (40000,)\n",
            "\n",
            "Processing chunk 5 | rows 1000000 to 1200009\n",
            "Saved shapes: (40000, 10, 78) (40000,)\n",
            "\n",
            "Processing chunk 6 | rows 1200000 to 1400009\n",
            "Saved shapes: (40000, 10, 78) (40000,)\n",
            "\n",
            "Processing chunk 7 | rows 1400000 to 1600009\n",
            "Saved shapes: (40000, 10, 78) (40000,)\n",
            "\n",
            "Processing chunk 8 | rows 1600000 to 1800009\n",
            "Saved shapes: (40000, 10, 78) (40000,)\n",
            "\n",
            "Processing chunk 9 | rows 1800000 to 2000009\n",
            "Saved shapes: (40000, 10, 78) (40000,)\n",
            "\n",
            "Processing chunk 10 | rows 2000000 to 2012223\n",
            "Saved shapes: (2443, 10, 78) (2443,)\n",
            "\n",
            "All chunks processed successfully.\n"
          ]
        }
      ],
      "source": [
        "output_dir = os.path.join(PROCESSED_DATA_DIR, \"sequence_chunks\")\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "chunk_id = 0\n",
        "\n",
        "for start in range(0, TOTAL_ROWS - SEQ_LEN + 1, CHUNK_SIZE):\n",
        "    end = min(start + CHUNK_SIZE + SEQ_LEN - 1, TOTAL_ROWS)\n",
        "\n",
        "    print(f\"\\nProcessing chunk {chunk_id} | rows {start} to {end}\")\n",
        "\n",
        "    X_chunk_seq, y_chunk_seq = process_chunk(\n",
        "        X_scaled, y, start, end, SEQ_LEN, stride=5\n",
        "    )\n",
        "\n",
        "    np.save(os.path.join(output_dir, f\"X_seq_chunk_{chunk_id}.npy\"), X_chunk_seq)\n",
        "    np.save(os.path.join(output_dir, f\"y_seq_chunk_{chunk_id}.npy\"), y_chunk_seq)\n",
        "\n",
        "    print(\"Saved shapes:\", X_chunk_seq.shape, y_chunk_seq.shape)\n",
        "\n",
        "    del X_chunk_seq, y_chunk_seq  # free RAM\n",
        "    chunk_id += 1\n",
        "\n",
        "print(\"\\nAll chunks processed successfully.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SFLngplC2iYP",
        "outputId": "dd44d831-2b64-4825-db2f-920209475c0a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total chunks created: 22\n",
            "Sample files: ['X_seq_chunk_0.npy', 'X_seq_chunk_1.npy', 'X_seq_chunk_10.npy', 'X_seq_chunk_2.npy']\n",
            "Total sequences created: 402443\n",
            "Expected: 2012214\n"
          ]
        }
      ],
      "source": [
        "chunk_files = sorted(os.listdir(output_dir))\n",
        "print(\"Total chunks created:\", len(chunk_files))\n",
        "print(\"Sample files:\", chunk_files[:4])\n",
        "\n",
        "total_sequences = 0\n",
        "\n",
        "for f in chunk_files:\n",
        "    if f.startswith(\"y_seq\"):\n",
        "        y_chunk = np.load(os.path.join(output_dir, f))\n",
        "        total_sequences += len(y_chunk)\n",
        "\n",
        "print(\"Total sequences created:\", total_sequences)\n",
        "print(\"Expected:\", TOTAL_ROWS - SEQ_LEN + 1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VaQ22aa4bZc8",
        "outputId": "2a58a8ef-054d-4aaf-8842-809523a715cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequence chunk directory exists: True\n",
            "Total files: 22\n",
            "Sample files: ['X_seq_chunk_0.npy', 'X_seq_chunk_1.npy', 'X_seq_chunk_10.npy', 'X_seq_chunk_2.npy', 'X_seq_chunk_3.npy', 'X_seq_chunk_4.npy']\n",
            "Total sequences found: 402443\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "\n",
        "SEQ_CHUNK_DIR = os.path.join(PROCESSED_DATA_DIR, \"sequence_chunks\")\n",
        "\n",
        "print(\"Sequence chunk directory exists:\", os.path.exists(SEQ_CHUNK_DIR))\n",
        "\n",
        "files = sorted(os.listdir(SEQ_CHUNK_DIR))\n",
        "print(\"Total files:\", len(files))\n",
        "print(\"Sample files:\", files[:6])\n",
        "\n",
        "total_sequences = 0\n",
        "\n",
        "for f in files:\n",
        "    if f.startswith(\"y_seq_chunk\"):\n",
        "        y_chunk = np.load(os.path.join(SEQ_CHUNK_DIR, f), mmap_mode=\"r\")\n",
        "        total_sequences += len(y_chunk)\n",
        "\n",
        "print(\"Total sequences found:\", total_sequences)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U6rqHQ6ScX7o"
      },
      "source": [
        "Phase 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WuTkwkRiFOu3",
        "outputId": "85b6a0b1-7edd-47d1-cec1-0e389b5f96c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "federated_clients cleared.\n"
          ]
        }
      ],
      "source": [
        "import shutil\n",
        "\n",
        "CLIENT_DATA_DIR = os.path.join(PROCESSED_DATA_DIR, \"federated_clients\")\n",
        "\n",
        "if os.path.exists(CLIENT_DATA_DIR):\n",
        "    shutil.rmtree(CLIENT_DATA_DIR)\n",
        "\n",
        "os.makedirs(CLIENT_DATA_DIR)\n",
        "\n",
        "for c in CLIENTS:\n",
        "    os.makedirs(os.path.join(CLIENT_DATA_DIR, c))\n",
        "\n",
        "print(\"federated_clients cleared.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "37RlYh56cXLL",
        "outputId": "a3a30ac4-479a-491e-c0c8-a45c98f6351c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total sequence chunks: 11\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "\n",
        "SEQ_CHUNK_DIR = os.path.join(PROCESSED_DATA_DIR, \"sequence_chunks\")\n",
        "assert os.path.exists(SEQ_CHUNK_DIR), \"sequence_chunks directory not found\"\n",
        "\n",
        "x_chunks = sorted([f for f in os.listdir(SEQ_CHUNK_DIR) if f.startswith(\"X_seq_chunk\")])\n",
        "y_chunks = sorted([f for f in os.listdir(SEQ_CHUNK_DIR) if f.startswith(\"y_seq_chunk\")])\n",
        "\n",
        "assert len(x_chunks) == len(y_chunks), \"Mismatch between X and y chunks\"\n",
        "\n",
        "NUM_CHUNKS = len(x_chunks)\n",
        "print(\"Total sequence chunks:\", NUM_CHUNKS)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S3JEpYXJgFEi",
        "outputId": "81322e77-5b9d-4ff1-e56b-6ee3c203149f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Federated clients initialized: ['Bank_A', 'Bank_B', 'Bank_C']\n"
          ]
        }
      ],
      "source": [
        "CLIENTS = [\"Bank_A\", \"Bank_B\", \"Bank_C\"]\n",
        "NUM_CLIENTS = len(CLIENTS)\n",
        "\n",
        "CLIENT_DATA_DIR = os.path.join(PROCESSED_DATA_DIR, \"federated_clients\")\n",
        "os.makedirs(CLIENT_DATA_DIR, exist_ok=True)\n",
        "\n",
        "for c in CLIENTS:\n",
        "    os.makedirs(os.path.join(CLIENT_DATA_DIR, c), exist_ok=True)\n",
        "\n",
        "print(\"Federated clients initialized:\", CLIENTS)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N5dgHKiSgH-Q"
      },
      "outputs": [],
      "source": [
        "DATA_DISTRIBUTION = \"iid\"\n",
        "# options: \"iid\", \"non_iid_label_skew\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ngw8NcffgKqp"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "def iid_partition(num_chunks, clients):\n",
        "    chunk_ids = list(range(num_chunks))\n",
        "    random.shuffle(chunk_ids)\n",
        "\n",
        "    assignments = {c: [] for c in clients}\n",
        "    for idx, cid in enumerate(chunk_ids):\n",
        "        assignments[clients[idx % len(clients)]].append(cid)\n",
        "\n",
        "    return assignments\n",
        "\n",
        "def non_iid_label_skew_partition(y_chunks, clients, dominance=0.7):\n",
        "\n",
        "    client_assignments = {c: [] for c in clients}\n",
        "\n",
        "    # analyze chunk labels\n",
        "    attack_chunks = []\n",
        "    benign_chunks = []\n",
        "\n",
        "    for i, y_file in enumerate(y_chunks):\n",
        "        y = np.load(os.path.join(SEQ_CHUNK_DIR, y_file), mmap_mode=\"r\")\n",
        "        attack_ratio = y.mean()  # since attack=1, benign=0\n",
        "\n",
        "        if attack_ratio > 0.5:\n",
        "            attack_chunks.append(i)\n",
        "        else:\n",
        "            benign_chunks.append(i)\n",
        "\n",
        "    random.shuffle(attack_chunks)\n",
        "    random.shuffle(benign_chunks)\n",
        "\n",
        "    for idx, client in enumerate(clients):\n",
        "        num_attack = int(dominance * len(attack_chunks) / len(clients))\n",
        "        num_benign = int((1 - dominance) * len(benign_chunks) / len(clients))\n",
        "\n",
        "        client_assignments[client].extend(attack_chunks[:num_attack])\n",
        "        client_assignments[client].extend(benign_chunks[:num_benign])\n",
        "\n",
        "        del attack_chunks[:num_attack]\n",
        "        del benign_chunks[:num_benign]\n",
        "\n",
        "    return client_assignments\n",
        "if DATA_DISTRIBUTION == \"iid\":\n",
        "    client_assignments = iid_partition(NUM_CHUNKS, CLIENTS)\n",
        "\n",
        "elif DATA_DISTRIBUTION == \"non_iid_label_skew\":\n",
        "    client_assignments = non_iid_label_skew_partition(y_chunks, CLIENTS)\n",
        "\n",
        "else:\n",
        "    raise ValueError(\"Unknown DATA_DISTRIBUTION\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8MFEoN8OgTdg",
        "outputId": "36a04507-f1e6-40d8-c08e-3a32094492d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Client-local datasets created successfully.\n"
          ]
        }
      ],
      "source": [
        "import shutil\n",
        "\n",
        "for client, chunk_ids in client_assignments.items():\n",
        "    client_path = os.path.join(CLIENT_DATA_DIR, client)\n",
        "\n",
        "    for cid in chunk_ids:\n",
        "        shutil.copy(\n",
        "            os.path.join(SEQ_CHUNK_DIR, x_chunks[cid]),\n",
        "            client_path\n",
        "        )\n",
        "        shutil.copy(\n",
        "            os.path.join(SEQ_CHUNK_DIR, y_chunks[cid]),\n",
        "            client_path\n",
        "        )\n",
        "\n",
        "print(\"Client-local datasets created successfully.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R4_iY8uZga64",
        "outputId": "5f5f93a6-5b17-428e-8af5-295034f1816a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Federated metadata saved at: /content/drive/MyDrive/FYP_FL_IDS/data/processed/federated_clients/federated_metadata.json\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "\n",
        "metadata = {\n",
        "    \"clients\": CLIENTS,\n",
        "    \"num_chunks\": NUM_CHUNKS,\n",
        "    \"data_distribution\": DATA_DISTRIBUTION,\n",
        "    \"client_assignments\": client_assignments\n",
        "}\n",
        "\n",
        "meta_path = os.path.join(CLIENT_DATA_DIR, \"federated_metadata.json\")\n",
        "\n",
        "with open(meta_path, \"w\") as f:\n",
        "    json.dump(metadata, f, indent=4)\n",
        "\n",
        "print(\"Federated metadata saved at:\", meta_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5CfxLD6rgdfo",
        "outputId": "1ea251f3-d17e-46ad-edfc-92db37639d1d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bank_A: X_chunks=4, y_chunks=4\n",
            "Bank_B: X_chunks=4, y_chunks=4\n",
            "Bank_C: X_chunks=3, y_chunks=3\n"
          ]
        }
      ],
      "source": [
        "for c in CLIENTS:\n",
        "    files = os.listdir(os.path.join(CLIENT_DATA_DIR, c))\n",
        "    x_files = [f for f in files if f.startswith(\"X_seq\")]\n",
        "    y_files = [f for f in files if f.startswith(\"y_seq\")]\n",
        "\n",
        "    print(f\"{c}: X_chunks={len(x_files)}, y_chunks={len(y_files)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6LDdD35c14Ne"
      },
      "source": [
        "Phase 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RtGE8L0m13bP",
        "outputId": "96eb96ec-5810-4fb2-ab91-eeeb55993087"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequence length: 10\n",
            "Number of features: 78\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "\n",
        "SEQ_CHUNK_DIR = os.path.join(PROCESSED_DATA_DIR, \"sequence_chunks\")\n",
        "\n",
        "\n",
        "sample_path = os.path.join(SEQ_CHUNK_DIR, \"X_seq_chunk_0.npy\")\n",
        "X_sample = np.load(sample_path, mmap_mode=\"r\")\n",
        "\n",
        "SEQ_LEN = CONFIG[\"SEQUENCE_LENGTH\"]\n",
        "NUM_FEATURES = X_sample.shape[2]\n",
        "\n",
        "print(\"Sequence length:\", SEQ_LEN)\n",
        "print(\"Number of features:\", NUM_FEATURES)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3tZpSzLs2Iqs"
      },
      "outputs": [],
      "source": [
        "class CNN_LSTM_IDS(nn.Module):\n",
        "    def __init__(self, seq_len, num_features):\n",
        "        super().__init__()\n",
        "\n",
        "        # Spatial feature extraction\n",
        "        self.conv1 = nn.Conv1d(\n",
        "            in_channels=num_features,\n",
        "            out_channels=64,\n",
        "            kernel_size=3,\n",
        "            padding=1\n",
        "        )\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "        # Temporal modeling\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=64,\n",
        "            hidden_size=64,\n",
        "            num_layers=1,\n",
        "            batch_first=True\n",
        "        )\n",
        "\n",
        "        # Classifier\n",
        "        self.fc = nn.Linear(64, 1)\n",
        "        #self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (B, T, F) -> Conv1d expects (B, F, T)\n",
        "        x = x.permute(0, 2, 1)\n",
        "        x = self.relu(self.conv1(x))\n",
        "        # back to (B, T, C)\n",
        "        x = x.permute(0, 2, 1)\n",
        "\n",
        "        _, (h_n, _) = self.lstm(x)\n",
        "        h_last = h_n[-1]           # (B, 64)\n",
        "        out = self.fc(h_last)\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-s0hU0e-2Noo",
        "outputId": "fb07b36d-04b8-4e1b-d2bd-7a1e41634dbb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CNN-LSTM parameters: 48385\n",
            "CNN-LSTM output shape: torch.Size([4, 1])\n"
          ]
        }
      ],
      "source": [
        "cnn_lstm_model = CNN_LSTM_IDS(SEQ_LEN, NUM_FEATURES).to(DEVICE)\n",
        "print(\"CNN-LSTM parameters:\", count_parameters(cnn_lstm_model))\n",
        "# Create a tiny dummy batch\n",
        "BATCH_TEST = 4\n",
        "dummy_x = torch.randn(BATCH_TEST, SEQ_LEN, NUM_FEATURES, device=DEVICE)\n",
        "\n",
        "with torch.no_grad():\n",
        "   # y_dnn = dnn_model(dummy_x)\n",
        "    y_cnnlstm = cnn_lstm_model(dummy_x)\n",
        "\n",
        "#print(\"DNN output shape:\", y_dnn.shape)\n",
        "print(\"CNN-LSTM output shape:\", y_cnnlstm.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5hiUvAdt3Cx0",
        "outputId": "a3cba7d5-d3de-4ee3-bf30-ec177df57351"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial model weights saved.\n"
          ]
        }
      ],
      "source": [
        "os.makedirs(MODELS_DIR, exist_ok=True)\n",
        "\n",
        "#torch.save(dnn_model.state_dict(), os.path.join(MODELS_DIR, \"dnn_init.pt\"))\n",
        "#torch.save(cnn_lstm_model.state_dict(), os.path.join(MODELS_DIR, \"cnn_lstm_init.pt\"))\n",
        "init_model_path = os.path.join(\n",
        "    MODELS_DIR, f\"cnn_lstm_init_{MODEL_VERSION}.pt\"\n",
        ")\n",
        "\n",
        "torch.save(cnn_lstm_model.state_dict(), init_model_path)\n",
        "\n",
        "print(\"Initial model weights saved.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MZecEhPz3Dbw",
        "outputId": "abc4e91b-ff39-4359-9d7b-7f6015aa3d90"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model metadata saved: /content/drive/MyDrive/FYP_FL_IDS/models/model_metadata.json\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "\n",
        "model_meta = {\n",
        "    \"sequence_length\": SEQ_LEN,\n",
        "    \"num_features\": NUM_FEATURES,\n",
        "    #\"dnn_params\": count_parameters(dnn_model),\n",
        "    \"cnn_lstm_params\": count_parameters(cnn_lstm_model)\n",
        "}\n",
        "\n",
        "meta_path = os.path.join(MODELS_DIR, \"model_metadata.json\")\n",
        "with open(meta_path, \"w\") as f:\n",
        "    json.dump(model_meta, f, indent=4)\n",
        "\n",
        "print(\"Model metadata saved:\", meta_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G3QiDrxv3Oso"
      },
      "source": [
        "phase 6"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S3OTnOiCOchR"
      },
      "source": [
        "## Phase 6: FedPHE - Packed Homomorphic Encryption Setup\n",
        "\n",
        "This phase implements **FedPHE (Federated Learning with Packed Homomorphic Encryption)** using TenSEAL.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6890f8f1"
      },
      "source": [
        "### Load and Evaluate Your Custom Saved Model\n",
        "\n",
        "First, define the path to your saved model and load its state dictionary into a new instance of the `CNN_LSTM_IDS` model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "smxrd0hk3QmY"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "import numpy as np\n",
        "import os\n",
        "import torch\n",
        "\n",
        "class ClientSequenceDataset(Dataset):\n",
        "    def __init__(self, client_dir):\n",
        "        self.x_files = sorted([\n",
        "            os.path.join(client_dir, f)\n",
        "            for f in os.listdir(client_dir) if f.startswith(\"X_seq\")\n",
        "        ])\n",
        "        self.y_files = sorted([\n",
        "            os.path.join(client_dir, f)\n",
        "            for f in os.listdir(client_dir) if f.startswith(\"y_seq\")\n",
        "        ])\n",
        "\n",
        "        assert len(self.x_files) == len(self.y_files)\n",
        "\n",
        "        self.chunk_sizes = []\n",
        "        for yf in self.y_files:\n",
        "            y = np.load(yf, mmap_mode=\"r\")\n",
        "            self.chunk_sizes.append(len(y))\n",
        "\n",
        "        self.cumulative_sizes = np.cumsum(self.chunk_sizes)\n",
        "\n",
        "\n",
        "        self._current_chunk_id = None\n",
        "        self._current_x = None\n",
        "        self._current_y = None\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(self.cumulative_sizes[-1])\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        chunk_id = np.searchsorted(self.cumulative_sizes, idx, side=\"right\")\n",
        "        local_idx = idx if chunk_id == 0 else idx - self.cumulative_sizes[chunk_id - 1]\n",
        "\n",
        "\n",
        "        if chunk_id != self._current_chunk_id:\n",
        "            self._current_x = np.load(self.x_files[chunk_id], mmap_mode=\"r\")\n",
        "            self._current_y = np.load(self.y_files[chunk_id], mmap_mode=\"r\")\n",
        "            self._current_chunk_id = chunk_id\n",
        "\n",
        "        x = self._current_x[local_idx]\n",
        "        y = self._current_y[local_idx]\n",
        "\n",
        "        return (\n",
        "            torch.tensor(x, dtype=torch.float32),\n",
        "            torch.tensor(y, dtype=torch.float32)\n",
        "        )\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C-1gILCK52e7"
      },
      "outputs": [],
      "source": [
        "MAX_BATCHES = 50\n",
        "\n",
        "def local_train(model, dataloader, epochs, lr):\n",
        "    model.train()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "    criterion = CRITERION\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        for batch_idx, (x, y) in enumerate(dataloader):\n",
        "            if batch_idx >= MAX_BATCHES:\n",
        "                break\n",
        "\n",
        "            x = x.to(DEVICE)\n",
        "            y = y.to(DEVICE).unsqueeze(1)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            preds = model(x)\n",
        "            loss = criterion(preds, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    return model.state_dict()\n",
        "\n",
        "\n",
        "\n",
        "def fedavg(state_dicts, data_sizes):\n",
        "    avg_state = {}\n",
        "    total = sum(data_sizes)\n",
        "\n",
        "    for key in state_dicts[0].keys():\n",
        "        avg_state[key] = sum(\n",
        "            state_dicts[i][key].float() * data_sizes[i] / total\n",
        "            for i in range(len(state_dicts))\n",
        "        )\n",
        "\n",
        "    return avg_state\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tAcge08c53Dj",
        "outputId": "2cf0c035-5c3b-4e89-e872-5bac8bbe3cc0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Global CNN-LSTM model initialized.\n"
          ]
        }
      ],
      "source": [
        "global_model = CNN_LSTM_IDS(SEQ_LEN, NUM_FEATURES).to(DEVICE)\n",
        "\n",
        "init_path = os.path.join(MODELS_DIR, \"cnn_lstm_init.pt\")\n",
        "global_model.load_state_dict(torch.load(init_path))\n",
        "\n",
        "print(\"Global CNN-LSTM model initialized.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jPRfz3O46FXV",
        "outputId": "ce713099-1e92-4018-dc9b-1f119206f6cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bank_A samples: 160000\n",
            "Bank_B samples: 160000\n",
            "Bank_C samples: 82443\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "CLIENT_DATA_DIR = os.path.join(PROCESSED_DATA_DIR, \"federated_clients\")\n",
        "CLIENTS = [\"Bank_A\", \"Bank_B\", \"Bank_C\"]\n",
        "\n",
        "client_loaders = {}\n",
        "client_sizes = {}\n",
        "\n",
        "for client in CLIENTS:\n",
        "    dataset = ClientSequenceDataset(\n",
        "        os.path.join(CLIENT_DATA_DIR, client)\n",
        "    )\n",
        "\n",
        "    loader = DataLoader(\n",
        "        dataset,\n",
        "        batch_size=CONFIG[\"BATCH_SIZE\"],\n",
        "        shuffle=True,\n",
        "        num_workers=0,\n",
        "        pin_memory=False\n",
        "    )\n",
        "\n",
        "    client_loaders[client] = loader\n",
        "    client_sizes[client] = len(dataset)\n",
        "\n",
        "    print(f\"{client} samples:\", len(dataset))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "321NI9rJb-D2"
      },
      "outputs": [],
      "source": [
        "\n",
        "eval_dataset = ClientSequenceDataset(\n",
        "    os.path.join(CLIENT_DATA_DIR, \"Bank_A\")\n",
        ")\n",
        "\n",
        "EVAL_SAMPLES = 5000  # keep small\n",
        "eval_indices = np.random.choice(len(eval_dataset), EVAL_SAMPLES, replace=False)\n",
        "\n",
        "eval_subset = torch.utils.data.Subset(eval_dataset, eval_indices)\n",
        "\n",
        "eval_loader = DataLoader(\n",
        "    eval_subset,\n",
        "    batch_size=256,\n",
        "    shuffle=False,\n",
        "    num_workers=0\n",
        ")\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score,\n",
        "    recall_score, f1_score, roc_auc_score\n",
        ")\n",
        "\n",
        "def evaluate_global_model(model, dataloader, threshold=0.3):\n",
        "    model.eval()\n",
        "\n",
        "    all_probs = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x, y in dataloader:\n",
        "            x = x.to(DEVICE)\n",
        "            y = y.to(DEVICE)\n",
        "\n",
        "            logits = model(x)\n",
        "            probs = torch.sigmoid(logits).cpu().numpy().ravel()\n",
        "\n",
        "            all_probs.extend(probs)\n",
        "            all_labels.extend(y.cpu().numpy())  # 🔑 FIX\n",
        "\n",
        "    all_probs = np.array(all_probs)\n",
        "    all_labels = np.array(all_labels)\n",
        "\n",
        "    preds = (all_probs > threshold).astype(int)\n",
        "\n",
        "    metrics = {\n",
        "        \"accuracy\": accuracy_score(all_labels, preds),\n",
        "        \"precision\": precision_score(all_labels, preds, zero_division=0),\n",
        "        \"recall\": recall_score(all_labels, preds, zero_division=0),\n",
        "        \"f1\": f1_score(all_labels, preds, zero_division=0),\n",
        "        \"roc_auc\": roc_auc_score(all_labels, all_probs)\n",
        "    }\n",
        "\n",
        "    return metrics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dQ4KGfF3OchS"
      },
      "outputs": [],
      "source": [
        "import tenseal as ts\n",
        "\n",
        "def create_ckks_context():\n",
        "    ctx = ts.context(\n",
        "        ts.SCHEME_TYPE.CKKS,\n",
        "        poly_modulus_degree=8192,\n",
        "        coeff_mod_bit_sizes=[60, 40, 40, 60]\n",
        "    )\n",
        "    ctx.global_scale = 2**30\n",
        "    ctx.generate_galois_keys()\n",
        "    return ctx\n",
        "\n",
        "\n",
        "ckks_ctx = create_ckks_context()\n",
        "\n",
        "\n",
        "public_ctx = ckks_ctx.copy()\n",
        "public_ctx.make_context_public()\n",
        "\n",
        "\n",
        "def compute_model_update(local_model, global_model):\n",
        "    delta = {}\n",
        "    global_state = global_model.state_dict()\n",
        "    local_state = local_model.state_dict()\n",
        "\n",
        "    for k in global_state:\n",
        "        if (\"lstm\" in k or \"fc\" in k) and (\"bias\" not in k):\n",
        "            delta[k] = local_state[k] - global_state[k]\n",
        "    return delta\n",
        "\n",
        "def encrypt_update(delta_state, ctx):\n",
        "    encrypted = []\n",
        "    shapes = []\n",
        "\n",
        "    for k, tensor in delta_state.items():\n",
        "        arr = tensor.detach().cpu().numpy()\n",
        "        shapes.append((k, arr.shape))\n",
        "        enc = ts.ckks_vector(ctx, arr.flatten())\n",
        "        encrypted.append(enc)\n",
        "\n",
        "    return encrypted, shapes\n",
        "def encrypted_sum(encrypted_updates):\n",
        "    agg = encrypted_updates[0]\n",
        "    for i in range(1, len(encrypted_updates)):\n",
        "        agg = [a + b for a, b in zip(agg, encrypted_updates[i])]\n",
        "    return agg\n",
        "\n",
        "\n",
        "def decrypt_update(enc_agg, shapes):\n",
        "    delta = {}\n",
        "    for enc, (k, shape) in zip(enc_agg, shapes):\n",
        "        dec = np.array(enc.decrypt())\n",
        "        delta[k] = torch.tensor(dec.reshape(shape), dtype=torch.float32)\n",
        "    return delta\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import tenseal as ts\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "\n",
        "HE_POLY_MODULUS = 16384\n",
        "HE_SCALE_BITS = 40\n",
        "HE_COEFF_MOD_BITS = [60, 40, 40, 40, 40, 60]\n",
        "\n",
        "# Selected layers to encrypt (60% faster)\n",
        "SELECTED_LAYERS = [\n",
        "    \"lstm.weight_ih_l0\",\n",
        "    \"lstm.weight_hh_l0\",\n",
        "    \"fc.weight\",\n",
        "    \"fc.bias\"\n",
        "]\n",
        "\n",
        "# ============================================================================\n",
        "# CREATE CKKS CONTEXT\n",
        "# ============================================================================\n",
        "\n",
        "def create_ckks_context():\n",
        "    \"\"\"Creates CKKS context - call ONCE before training\"\"\"\n",
        "    context = ts.context(\n",
        "        ts.SCHEME_TYPE.CKKS,\n",
        "        poly_modulus_degree=HE_POLY_MODULUS,\n",
        "        coeff_mod_bit_sizes=HE_COEFF_MOD_BITS\n",
        "    )\n",
        "    context.global_scale = 2 ** HE_SCALE_BITS\n",
        "    context.generate_galois_keys()\n",
        "    return context\n",
        "\n",
        "# ============================================================================\n",
        "# COMPUTE MODEL UPDATE (ΔW)\n",
        "# ============================================================================\n",
        "\n",
        "def compute_model_update(local_model, global_model):\n",
        "    \"\"\"Computes ΔW = W_local - W_global\"\"\"\n",
        "    delta = {}\n",
        "    local_state = local_model.state_dict()\n",
        "    global_state = global_model.state_dict()\n",
        "\n",
        "    for key in SELECTED_LAYERS:\n",
        "        if key in local_state and key in global_state:\n",
        "            diff = local_state[key] - global_state[key]\n",
        "            # Clip extreme values\n",
        "            diff = torch.clamp(diff, min=-10.0, max=10.0)\n",
        "            delta[key] = diff\n",
        "\n",
        "    return delta\n",
        "\n",
        "# ============================================================================\n",
        "# ENCRYPT UPDATE\n",
        "# ============================================================================\n",
        "\n",
        "def encrypt_update(delta, context):\n",
        "    \"\"\"Encrypts weight delta\"\"\"\n",
        "    encrypted = {}\n",
        "    shapes = {}\n",
        "\n",
        "    for key, tensor in delta.items():\n",
        "        shapes[key] = tensor.shape\n",
        "        flat = tensor.cpu().detach().numpy().flatten()\n",
        "\n",
        "        # Validate\n",
        "        if np.isnan(flat).any() or np.isinf(flat).any():\n",
        "            flat = np.nan_to_num(flat, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "\n",
        "        # Clip\n",
        "        flat = np.clip(flat, -10.0, 10.0)\n",
        "\n",
        "        # Encrypt\n",
        "        encrypted[key] = ts.ckks_vector(context, flat.tolist())\n",
        "\n",
        "    return encrypted, shapes\n",
        "\n",
        "# ============================================================================\n",
        "# ENCRYPTED SUM (SERVER-SIDE AGGREGATION)\n",
        "# ============================================================================\n",
        "\n",
        "def encrypted_sum(encrypted_updates):\n",
        "    \"\"\"Sums encrypted updates from all clients\"\"\"\n",
        "    if not encrypted_updates:\n",
        "        return {}\n",
        "\n",
        "    result = {}\n",
        "    all_keys = encrypted_updates[0].keys()\n",
        "\n",
        "    for key in all_keys:\n",
        "        # Start with first client\n",
        "        result[key] = encrypted_updates[0][key]\n",
        "\n",
        "        # Add remaining clients\n",
        "        for i in range(1, len(encrypted_updates)):\n",
        "            result[key] = result[key] + encrypted_updates[i][key]\n",
        "\n",
        "    return result\n",
        "\n",
        "# ============================================================================\n",
        "# DECRYPT UPDATE\n",
        "# ============================================================================\n",
        "\n",
        "def decrypt_update(encrypted_sum, shapes):\n",
        "    \"\"\"Decrypts aggregated update\"\"\"\n",
        "    decrypted = {}\n",
        "\n",
        "    for key, enc_vec in encrypted_sum.items():\n",
        "        # Decrypt\n",
        "        flat = enc_vec.decrypt()\n",
        "        flat = np.array(flat, dtype=np.float32)\n",
        "\n",
        "        # Validate\n",
        "        flat = np.nan_to_num(flat, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "\n",
        "        # Reshape\n",
        "        shape = shapes[key]\n",
        "        num_elements = np.prod(shape)\n",
        "        flat = flat[:num_elements]\n",
        "\n",
        "        tensor = torch.tensor(flat, dtype=torch.float32)\n",
        "        tensor = tensor.reshape(shape)\n",
        "\n",
        "        decrypted[key] = tensor\n",
        "\n",
        "    return decrypted\n",
        "\n",
        "# ============================================================================\n",
        "# USAGE IN YOUR TRAINING LOOP:\n",
        "# ============================================================================\n",
        "\n",
        "# BEFORE training loop:\n",
        "# ckks_ctx = create_ckks_context()\n",
        "\n",
        "# INSIDE your loop - NO CHANGES NEEDED to your code!\n",
        "# Your code already calls these functions correctly."
      ],
      "metadata": {
        "id": "aK6oR7a2TLWL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "\n",
        "CHECKPOINT_DIR = os.path.join(BASE_DIR, \"fedphe_checkpoints1\")\n",
        "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
        "\n",
        "def save_checkpoint(round_idx, model):\n",
        "    ckpt_path = os.path.join(CHECKPOINT_DIR, f\"fedphe_round_{round_idx}.pt\")\n",
        "    torch.save({\n",
        "        \"round\": round_idx,\n",
        "        \"model_state\": model.state_dict()\n",
        "    }, ckpt_path)\n",
        "    print(f\" Checkpoint saved: {ckpt_path}\")\n",
        "\n",
        "\n",
        "def load_latest_checkpoint(model):\n",
        "    if not os.path.exists(CHECKPOINT_DIR):\n",
        "        return 0  # start from scratch\n",
        "\n",
        "    ckpts = [f for f in os.listdir(CHECKPOINT_DIR) if f.endswith(\".pt\")]\n",
        "    if not ckpts:\n",
        "        return 0\n",
        "\n",
        "    ckpts.sort(key=lambda x: int(x.split(\"_\")[-1].split(\".\")[0]))\n",
        "    latest_ckpt = ckpts[-1]\n",
        "\n",
        "    ckpt_path = os.path.join(CHECKPOINT_DIR, latest_ckpt)\n",
        "    data = torch.load(ckpt_path, map_location=DEVICE)\n",
        "\n",
        "    model.load_state_dict(data[\"model_state\"])\n",
        "    start_round = data[\"round\"] + 1\n",
        "\n",
        "    print(f\" Resuming from checkpoint: {ckpt_path}\")\n",
        "    print(f\"  Starting from round {start_round+1}\")\n",
        "\n",
        "    return start_round\n"
      ],
      "metadata": {
        "id": "yXrRp5yjVYBj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ckks_ctx = create_ckks_context()\n",
        "ROUNDS = CONFIG[\"ROUNDS\"]\n",
        "\n",
        "start_round = load_latest_checkpoint(global_model)\n",
        "\n",
        "for rnd in range(start_round, ROUNDS):\n",
        "    print(f\"\\n===== Federated Round {rnd+1}/{ROUNDS} =====\")\n",
        "\n",
        "    encrypted_updates = []\n",
        "\n",
        "\n",
        "    for client in CLIENTS:\n",
        "        print(f\"Training locally on {client}...\")\n",
        "\n",
        "        local_model = CNN_LSTM_IDS(SEQ_LEN, NUM_FEATURES).to(DEVICE)\n",
        "        local_model.load_state_dict(global_model.state_dict())\n",
        "\n",
        "        local_train(\n",
        "            local_model,\n",
        "            client_loaders[client],\n",
        "            CONFIG[\"LOCAL_EPOCHS\"],\n",
        "            CONFIG[\"LEARNING_RATE\"]\n",
        "        )\n",
        "\n",
        "        delta = compute_model_update(local_model, global_model)\n",
        "        enc_delta, shapes = encrypt_update(delta, ckks_ctx)\n",
        "        encrypted_updates.append(enc_delta)\n",
        "\n",
        "        del local_model\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "\n",
        "    enc_sum = encrypted_sum(encrypted_updates)\n",
        "\n",
        "\n",
        "    delta_avg = decrypt_update(enc_sum, shapes)\n",
        "\n",
        "    for k in delta_avg:\n",
        "        delta_avg[k] /= len(CLIENTS)\n",
        "\n",
        "    if rnd == 0:\n",
        "        norm = sum(torch.norm(v).item() for v in delta_avg.values())\n",
        "        print(\"ΔW norm (sanity):\", norm)\n",
        "\n",
        "    current_state = global_model.state_dict()\n",
        "    for k in delta_avg:\n",
        "        current_state[k] += delta_avg[k].to(DEVICE)\n",
        "\n",
        "    global_model.load_state_dict(current_state)\n",
        "\n",
        "    print(\"Global model updated.\")\n",
        "\n",
        "\n",
        "    CHECKPOINT_FREQ = 2\n",
        "    if (rnd + 1) % CHECKPOINT_FREQ == 0:\n",
        "        save_checkpoint(rnd, global_model)\n",
        "\n",
        "    import gc\n",
        "    gc.collect()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jQklalklVTj0",
        "outputId": "55bc725e-c5da-4431-afab-f7b5d70d1559"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔁 Resuming from checkpoint: /content/drive/MyDrive/FYP_FL_IDS/fedphe_checkpoints1/fedphe_round_19.pt\n",
            "➡️  Starting from round 21\n",
            "\n",
            "===== Federated Round 21/25 =====\n",
            "Training locally on Bank_A...\n",
            "WARNING: The input does not fit in a single ciphertext, and some operations will be disabled.\n",
            "The following operations are disabled in this setup: matmul, matmul_plain, enc_matmul_plain, conv2d_im2col.\n",
            "If you need to use those operations, try increasing the poly_modulus parameter, to fit your input.\n",
            "WARNING: The input does not fit in a single ciphertext, and some operations will be disabled.\n",
            "The following operations are disabled in this setup: matmul, matmul_plain, enc_matmul_plain, conv2d_im2col.\n",
            "If you need to use those operations, try increasing the poly_modulus parameter, to fit your input.\n",
            "Training locally on Bank_B...\n",
            "WARNING: The input does not fit in a single ciphertext, and some operations will be disabled.\n",
            "The following operations are disabled in this setup: matmul, matmul_plain, enc_matmul_plain, conv2d_im2col.\n",
            "If you need to use those operations, try increasing the poly_modulus parameter, to fit your input.\n",
            "WARNING: The input does not fit in a single ciphertext, and some operations will be disabled.\n",
            "The following operations are disabled in this setup: matmul, matmul_plain, enc_matmul_plain, conv2d_im2col.\n",
            "If you need to use those operations, try increasing the poly_modulus parameter, to fit your input.\n",
            "Training locally on Bank_C...\n",
            "WARNING: The input does not fit in a single ciphertext, and some operations will be disabled.\n",
            "The following operations are disabled in this setup: matmul, matmul_plain, enc_matmul_plain, conv2d_im2col.\n",
            "If you need to use those operations, try increasing the poly_modulus parameter, to fit your input.\n",
            "WARNING: The input does not fit in a single ciphertext, and some operations will be disabled.\n",
            "The following operations are disabled in this setup: matmul, matmul_plain, enc_matmul_plain, conv2d_im2col.\n",
            "If you need to use those operations, try increasing the poly_modulus parameter, to fit your input.\n",
            "Global model updated.\n",
            "\n",
            "===== Federated Round 22/25 =====\n",
            "Training locally on Bank_A...\n",
            "WARNING: The input does not fit in a single ciphertext, and some operations will be disabled.\n",
            "The following operations are disabled in this setup: matmul, matmul_plain, enc_matmul_plain, conv2d_im2col.\n",
            "If you need to use those operations, try increasing the poly_modulus parameter, to fit your input.\n",
            "WARNING: The input does not fit in a single ciphertext, and some operations will be disabled.\n",
            "The following operations are disabled in this setup: matmul, matmul_plain, enc_matmul_plain, conv2d_im2col.\n",
            "If you need to use those operations, try increasing the poly_modulus parameter, to fit your input.\n",
            "Training locally on Bank_B...\n",
            "WARNING: The input does not fit in a single ciphertext, and some operations will be disabled.\n",
            "The following operations are disabled in this setup: matmul, matmul_plain, enc_matmul_plain, conv2d_im2col.\n",
            "If you need to use those operations, try increasing the poly_modulus parameter, to fit your input.\n",
            "WARNING: The input does not fit in a single ciphertext, and some operations will be disabled.\n",
            "The following operations are disabled in this setup: matmul, matmul_plain, enc_matmul_plain, conv2d_im2col.\n",
            "If you need to use those operations, try increasing the poly_modulus parameter, to fit your input.\n",
            "Training locally on Bank_C...\n",
            "WARNING: The input does not fit in a single ciphertext, and some operations will be disabled.\n",
            "The following operations are disabled in this setup: matmul, matmul_plain, enc_matmul_plain, conv2d_im2col.\n",
            "If you need to use those operations, try increasing the poly_modulus parameter, to fit your input.\n",
            "WARNING: The input does not fit in a single ciphertext, and some operations will be disabled.\n",
            "The following operations are disabled in this setup: matmul, matmul_plain, enc_matmul_plain, conv2d_im2col.\n",
            "If you need to use those operations, try increasing the poly_modulus parameter, to fit your input.\n",
            "Global model updated.\n",
            "💾 Checkpoint saved: /content/drive/MyDrive/FYP_FL_IDS/fedphe_checkpoints1/fedphe_round_21.pt\n",
            "\n",
            "===== Federated Round 23/25 =====\n",
            "Training locally on Bank_A...\n",
            "WARNING: The input does not fit in a single ciphertext, and some operations will be disabled.\n",
            "The following operations are disabled in this setup: matmul, matmul_plain, enc_matmul_plain, conv2d_im2col.\n",
            "If you need to use those operations, try increasing the poly_modulus parameter, to fit your input.\n",
            "WARNING: The input does not fit in a single ciphertext, and some operations will be disabled.\n",
            "The following operations are disabled in this setup: matmul, matmul_plain, enc_matmul_plain, conv2d_im2col.\n",
            "If you need to use those operations, try increasing the poly_modulus parameter, to fit your input.\n",
            "Training locally on Bank_B...\n",
            "WARNING: The input does not fit in a single ciphertext, and some operations will be disabled.\n",
            "The following operations are disabled in this setup: matmul, matmul_plain, enc_matmul_plain, conv2d_im2col.\n",
            "If you need to use those operations, try increasing the poly_modulus parameter, to fit your input.\n",
            "WARNING: The input does not fit in a single ciphertext, and some operations will be disabled.\n",
            "The following operations are disabled in this setup: matmul, matmul_plain, enc_matmul_plain, conv2d_im2col.\n",
            "If you need to use those operations, try increasing the poly_modulus parameter, to fit your input.\n",
            "Training locally on Bank_C...\n",
            "WARNING: The input does not fit in a single ciphertext, and some operations will be disabled.\n",
            "The following operations are disabled in this setup: matmul, matmul_plain, enc_matmul_plain, conv2d_im2col.\n",
            "If you need to use those operations, try increasing the poly_modulus parameter, to fit your input.\n",
            "WARNING: The input does not fit in a single ciphertext, and some operations will be disabled.\n",
            "The following operations are disabled in this setup: matmul, matmul_plain, enc_matmul_plain, conv2d_im2col.\n",
            "If you need to use those operations, try increasing the poly_modulus parameter, to fit your input.\n",
            "Global model updated.\n",
            "\n",
            "===== Federated Round 24/25 =====\n",
            "Training locally on Bank_A...\n",
            "WARNING: The input does not fit in a single ciphertext, and some operations will be disabled.\n",
            "The following operations are disabled in this setup: matmul, matmul_plain, enc_matmul_plain, conv2d_im2col.\n",
            "If you need to use those operations, try increasing the poly_modulus parameter, to fit your input.\n",
            "WARNING: The input does not fit in a single ciphertext, and some operations will be disabled.\n",
            "The following operations are disabled in this setup: matmul, matmul_plain, enc_matmul_plain, conv2d_im2col.\n",
            "If you need to use those operations, try increasing the poly_modulus parameter, to fit your input.\n",
            "Training locally on Bank_B...\n",
            "WARNING: The input does not fit in a single ciphertext, and some operations will be disabled.\n",
            "The following operations are disabled in this setup: matmul, matmul_plain, enc_matmul_plain, conv2d_im2col.\n",
            "If you need to use those operations, try increasing the poly_modulus parameter, to fit your input.\n",
            "WARNING: The input does not fit in a single ciphertext, and some operations will be disabled.\n",
            "The following operations are disabled in this setup: matmul, matmul_plain, enc_matmul_plain, conv2d_im2col.\n",
            "If you need to use those operations, try increasing the poly_modulus parameter, to fit your input.\n",
            "Training locally on Bank_C...\n",
            "WARNING: The input does not fit in a single ciphertext, and some operations will be disabled.\n",
            "The following operations are disabled in this setup: matmul, matmul_plain, enc_matmul_plain, conv2d_im2col.\n",
            "If you need to use those operations, try increasing the poly_modulus parameter, to fit your input.\n",
            "WARNING: The input does not fit in a single ciphertext, and some operations will be disabled.\n",
            "The following operations are disabled in this setup: matmul, matmul_plain, enc_matmul_plain, conv2d_im2col.\n",
            "If you need to use those operations, try increasing the poly_modulus parameter, to fit your input.\n",
            "Global model updated.\n",
            "💾 Checkpoint saved: /content/drive/MyDrive/FYP_FL_IDS/fedphe_checkpoints1/fedphe_round_23.pt\n",
            "\n",
            "===== Federated Round 25/25 =====\n",
            "Training locally on Bank_A...\n",
            "WARNING: The input does not fit in a single ciphertext, and some operations will be disabled.\n",
            "The following operations are disabled in this setup: matmul, matmul_plain, enc_matmul_plain, conv2d_im2col.\n",
            "If you need to use those operations, try increasing the poly_modulus parameter, to fit your input.\n",
            "WARNING: The input does not fit in a single ciphertext, and some operations will be disabled.\n",
            "The following operations are disabled in this setup: matmul, matmul_plain, enc_matmul_plain, conv2d_im2col.\n",
            "If you need to use those operations, try increasing the poly_modulus parameter, to fit your input.\n",
            "Training locally on Bank_B...\n",
            "WARNING: The input does not fit in a single ciphertext, and some operations will be disabled.\n",
            "The following operations are disabled in this setup: matmul, matmul_plain, enc_matmul_plain, conv2d_im2col.\n",
            "If you need to use those operations, try increasing the poly_modulus parameter, to fit your input.\n",
            "WARNING: The input does not fit in a single ciphertext, and some operations will be disabled.\n",
            "The following operations are disabled in this setup: matmul, matmul_plain, enc_matmul_plain, conv2d_im2col.\n",
            "If you need to use those operations, try increasing the poly_modulus parameter, to fit your input.\n",
            "Training locally on Bank_C...\n",
            "WARNING: The input does not fit in a single ciphertext, and some operations will be disabled.\n",
            "The following operations are disabled in this setup: matmul, matmul_plain, enc_matmul_plain, conv2d_im2col.\n",
            "If you need to use those operations, try increasing the poly_modulus parameter, to fit your input.\n",
            "WARNING: The input does not fit in a single ciphertext, and some operations will be disabled.\n",
            "The following operations are disabled in this setup: matmul, matmul_plain, enc_matmul_plain, conv2d_im2col.\n",
            "If you need to use those operations, try increasing the poly_modulus parameter, to fit your input.\n",
            "Global model updated.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6h_xK0ig6RPy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        },
        "outputId": "6dca32b0-262a-4398-c248-59a5d15178ee"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'create_ckks_context' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2233174689.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#OLD\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mckks_ctx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_ckks_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mROUNDS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCONFIG\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"ROUNDS\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'create_ckks_context' is not defined"
          ]
        }
      ],
      "source": [
        "#OLD\n",
        "\n",
        "ckks_ctx = create_ckks_context()\n",
        "ROUNDS = CONFIG[\"ROUNDS\"]\n",
        "\n",
        "for rnd in range(ROUNDS):\n",
        "    print(f\"\\n===== Federated Round {rnd+1}/{ROUNDS} =====\")\n",
        "\n",
        "    encrypted_updates = []   # 🔐 collect ALL client updates\n",
        "\n",
        "    # -------- CLIENT SIDE --------\n",
        "    for client in CLIENTS:\n",
        "        print(f\"Training locally on {client}...\")\n",
        "\n",
        "        local_model = CNN_LSTM_IDS(SEQ_LEN, NUM_FEATURES).to(DEVICE)\n",
        "        local_model.load_state_dict(global_model.state_dict())\n",
        "\n",
        "        local_train(\n",
        "            local_model,\n",
        "            client_loaders[client],\n",
        "            CONFIG[\"LOCAL_EPOCHS\"],\n",
        "            CONFIG[\"LEARNING_RATE\"]\n",
        "        )\n",
        "\n",
        "        # 🔐 compute & encrypt UPDATE (ΔW)\n",
        "        delta = compute_model_update(local_model, global_model)\n",
        "        enc_delta, shapes = encrypt_update(delta, ckks_ctx)\n",
        "\n",
        "        encrypted_updates.append(enc_delta)\n",
        "\n",
        "        del local_model\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    # -------- SERVER SIDE (BLIND) --------\n",
        "    enc_sum = encrypted_sum(encrypted_updates)\n",
        "\n",
        "    # -------- CLIENT SIDE (DECRYPT + APPLY) --------\n",
        "    delta_avg = decrypt_update(enc_sum, shapes)\n",
        "\n",
        "    # Average\n",
        "    for k in delta_avg:\n",
        "        delta_avg[k] /= len(CLIENTS)\n",
        "\n",
        "    # 🔎 Sanity check (only once)\n",
        "    if rnd == 0:\n",
        "        norm = sum(torch.norm(v).item() for v in delta_avg.values())\n",
        "        print(\"ΔW norm (sanity):\", norm)\n",
        "\n",
        "    # Apply update\n",
        "    current_state = global_model.state_dict()\n",
        "    for k in delta_avg:\n",
        "        current_state[k] += delta_avg[k].to(DEVICE)\n",
        "\n",
        "    global_model.load_state_dict(current_state)\n",
        "\n",
        "    print(\"Global model updated.\")\n",
        "\n",
        "    import gc\n",
        "    gc.collect()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "APImKrYZgcon",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c827335-45f2-471a-cd85-36cce1e83f3f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Plaintext FL model saved at: /content/drive/MyDrive/FYP_FL_IDS/models/cnn_lstm_global_with_HE_25rounds_16k.pt\n"
          ]
        }
      ],
      "source": [
        "final_model_path = os.path.join(\n",
        "    MODELS_DIR, f\"cnn_lstm_global_with_HE_25rounds_16k.pt\"\n",
        ")\n",
        "\n",
        "torch.save(global_model.state_dict(), final_model_path)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(\"Plaintext FL model saved at:\", final_model_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score,\n",
        "    f1_score, roc_auc_score, confusion_matrix\n",
        ")\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Instantiate model (MUST match training architecture)\n",
        "model = CNN_LSTM_IDS(SEQ_LEN, NUM_FEATURES).to(DEVICE)\n",
        "\n",
        "# Load checkpoint or final model\n",
        "MODEL_PATH = \"/content/drive/MyDrive/FYP_FL_IDS/models/cnn_lstm_global_with_HE_25rounds_16k.pt\"\n",
        "# OR checkpoint: fedphe_checkpoints/fedphe_round_8.pt\n",
        "\n",
        "state = torch.load(MODEL_PATH, map_location=DEVICE)\n",
        "\n",
        "# Handle checkpoint vs final model\n",
        "if isinstance(state, dict) and \"model_state\" in state:\n",
        "    model.load_state_dict(state[\"model_state\"])\n",
        "else:\n",
        "    model.load_state_dict(state)\n",
        "\n",
        "model.eval()\n",
        "print(\"✅ FedPHE model loaded for evaluation\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_kFDKhqTTmBn",
        "outputId": "7863e0e4-0bdd-41c2-85b7-b91c6014cb22"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ FedPHE model loaded for evaluation\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, dataloader, threshold=0.3):\n",
        "    y_true, y_pred, y_prob = [], [], []\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for x, y in dataloader:\n",
        "            x = x.to(DEVICE)\n",
        "            y = y.cpu().numpy()\n",
        "\n",
        "            logits = model(x)\n",
        "            probs = torch.sigmoid(logits).cpu().numpy().flatten()\n",
        "            preds = (probs > threshold).astype(int)\n",
        "\n",
        "            y_true.extend(y)\n",
        "            y_pred.extend(preds)\n",
        "            y_prob.extend(probs)\n",
        "\n",
        "    metrics = {\n",
        "        \"accuracy\": accuracy_score(y_true, y_pred),\n",
        "        \"precision\": precision_score(y_true, y_pred),\n",
        "        \"recall\": recall_score(y_true, y_pred),\n",
        "        \"f1\": f1_score(y_true, y_pred),\n",
        "        \"roc_auc\": roc_auc_score(y_true, y_prob),\n",
        "        \"confusion_matrix\": confusion_matrix(y_true, y_pred)\n",
        "    }\n",
        "\n",
        "    return metrics\n"
      ],
      "metadata": {
        "id": "eOJ-vd-vk6t7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "THRESHOLD = 0.5 # good for IDS demos\n",
        "\n",
        "metrics = evaluate_model(model, eval_loader, threshold=THRESHOLD)\n",
        "\n",
        "print(\"\\n📊 FedPHE Evaluation Results\")\n",
        "print(\"=\"*40)\n",
        "print(f\"Accuracy:  {metrics['accuracy']:.4f}\")\n",
        "print(f\"Precision: {metrics['precision']:.4f}\")\n",
        "print(f\"Recall:    {metrics['recall']:.4f}\")\n",
        "print(f\"F1-score:  {metrics['f1']:.4f}\")\n",
        "print(f\"ROC-AUC:   {metrics['roc_auc']:.4f}\")\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(metrics[\"confusion_matrix\"])\n",
        "print(\"=\"*40)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qqltam3Pk9n4",
        "outputId": "114053ff-0cf7-4b87-c2ef-976414b4e80a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "📊 FedPHE Evaluation Results\n",
            "========================================\n",
            "Accuracy:  0.9782\n",
            "Precision: 0.9838\n",
            "Recall:    0.9522\n",
            "F1-score:  0.9678\n",
            "ROC-AUC:   0.9958\n",
            "\n",
            "Confusion Matrix:\n",
            "[[104158    861]\n",
            " [  2626  52355]]\n",
            "========================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def load_saved_model(model_path, model_class, seq_len, num_features):\n",
        "    \"\"\"\n",
        "    model_path: path to .pt file\n",
        "    model_class: CNN_LSTM_IDS\n",
        "    \"\"\"\n",
        "    model = model_class(seq_len, num_features).to(DEVICE)\n",
        "\n",
        "    state = torch.load(model_path, map_location=DEVICE)\n",
        "\n",
        "    # Handle checkpoint or plain model\n",
        "    if isinstance(state, dict) and \"model_state\" in state:\n",
        "        model.load_state_dict(state[\"model_state\"])\n",
        "    else:\n",
        "        model.load_state_dict(state)\n",
        "\n",
        "    model.eval()\n",
        "    print(f\"✅ Loaded model: {model_path}\")\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "dONlShCLdLy6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "SEQ_LEN = 10\n",
        "NUM_FEATURES = 78  # FIXED\n",
        "\n",
        "benign_normal = np.random.normal(\n",
        "    loc=0.05,\n",
        "    scale=0.05,\n",
        "    size=(SEQ_LEN, NUM_FEATURES)\n",
        ")\n",
        "\n",
        "attack_ddos = np.random.normal(\n",
        "    loc=1.2,\n",
        "    scale=0.8,\n",
        "    size=(SEQ_LEN, NUM_FEATURES)\n",
        ")\n",
        "\n",
        "# burst spikes (packet count, byte count, flow rate)\n",
        "attack_ddos[:, :6] += 3.5\n",
        "attack_ddos[:, 12:18] += 2.0\n",
        "\n",
        "attack_slow = np.random.normal(\n",
        "    loc=0.4,\n",
        "    scale=0.25,\n",
        "    size=(SEQ_LEN, NUM_FEATURES)\n",
        ")\n",
        "\n",
        "# temporal accumulation\n",
        "attack_slow = np.cumsum(attack_slow, axis=0)\n",
        "\n",
        "# protocol misuse\n",
        "attack_slow[:, 20:26] += 1.5\n",
        "\n",
        "attack_portscan = np.random.normal(\n",
        "    loc=0.3,\n",
        "    scale=0.6,\n",
        "    size=(SEQ_LEN, NUM_FEATURES)\n",
        ")\n",
        "\n",
        "# many short-lived flows\n",
        "attack_portscan[:, 30:40] += 2.5\n",
        "attack_portscan[:, 5:10] -= 0.2\n",
        "\n",
        "attack_bruteforce = np.random.normal(\n",
        "    loc=0.5,\n",
        "    scale=0.3,\n",
        "    size=(SEQ_LEN, NUM_FEATURES)\n",
        ")\n",
        "\n",
        "# repeated authentication failures\n",
        "attack_bruteforce[:, 45:50] += 3.0\n",
        "attack_bruteforce[:, 0:2] += 1.2\n",
        "\n",
        "attack_botnet = np.random.normal(\n",
        "    loc=0.7,\n",
        "    scale=0.15,\n",
        "    size=(SEQ_LEN, NUM_FEATURES)\n",
        ")\n",
        "\n",
        "# periodic beacons\n",
        "for t in range(SEQ_LEN):\n",
        "    attack_botnet[t, 60:65] += (t % 2) * 2.5\n",
        "\n",
        "attack_exfiltration = np.random.normal(\n",
        "    loc=0.4,\n",
        "    scale=0.2,\n",
        "    size=(SEQ_LEN, NUM_FEATURES)\n",
        ")\n",
        "\n",
        "# sustained payload size\n",
        "attack_exfiltration[:, 70:75] += 2.8\n",
        "attack_exfiltration[:, 15:18] += 1.2\n",
        "\n",
        "attack_hybrid = np.random.normal(\n",
        "    loc=0.6,\n",
        "    scale=0.5,\n",
        "    size=(SEQ_LEN, NUM_FEATURES)\n",
        ")\n",
        "\n",
        "# combine behaviors\n",
        "attack_hybrid[:, :5] += 2.5         # burst\n",
        "attack_hybrid[:, 20:25] += 1.5      # protocol anomaly\n",
        "attack_hybrid[:, 45:50] += 2.0      # auth failures\n",
        "\n",
        "def test_all_attacks(model, threshold=0.5):\n",
        "    attacks = {\n",
        "        \"Benign\": benign_normal,\n",
        "        \"DDoS\": attack_ddos,\n",
        "        \"Slow Attack\": attack_slow,\n",
        "        \"Port Scan\": attack_portscan,\n",
        "        \"Brute Force\": attack_bruteforce,\n",
        "        \"Botnet\": attack_botnet,\n",
        "        \"Exfiltration\": attack_exfiltration,\n",
        "        \"Hybrid\": attack_hybrid,\n",
        "    }\n",
        "\n",
        "    print(\"=\"*60)\n",
        "    for name, sample in attacks.items():\n",
        "        prob, pred = predict_sample(sample, model, threshold)\n",
        "        print(f\"{name:<15} → {pred:<10} | prob={prob:.4f}\")\n",
        "    print(\"=\"*60)\n",
        "\n"
      ],
      "metadata": {
        "id": "IY6AY_ZwdHyw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_sample(sample, model, threshold=0.3):\n",
        "    x = torch.tensor(sample, dtype=torch.float32).unsqueeze(0).to(DEVICE)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        logit = model(x)\n",
        "        prob = torch.sigmoid(logit).item()\n",
        "\n",
        "    label = \"ATTACK 🚨\" if prob > threshold else \"BENIGN ✅\"\n",
        "    return prob, label\n"
      ],
      "metadata": {
        "id": "-QPGGdACdTU_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_PATHS = {\n",
        "    \"HE_10_rounds_16k\":\"/content/drive/MyDrive/FYP_FL_IDS/models/cnn_lstm_global_with_HE_12rounds_16k.pt\",\n",
        "    \"16k20 rounds\":\"/content/drive/MyDrive/FYP_FL_IDS/models/cnn_lstm_global_with_HE_20rounds_16k.pt\",\n",
        "    \"HE_25_rounds_16k\": \"/content/drive/MyDrive/FYP_FL_IDS/models/cnn_lstm_global_with_HE_25rounds_16k.pt\",\n",
        "}\n"
      ],
      "metadata": {
        "id": "FQfhXBl5eQY3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "THRESHOLD = 0.5\n",
        "\n",
        "for name, path in MODEL_PATHS.items():\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(f\"🔍 Testing model: {name}\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    model = load_saved_model(\n",
        "        path,\n",
        "        CNN_LSTM_IDS,\n",
        "        SEQ_LEN,\n",
        "        NUM_FEATURES\n",
        "    )\n",
        "    test_all_attacks(model, threshold=THRESHOLD)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZdZw2kZLeTW4",
        "outputId": "6ab78991-1a88-4117-faa3-3eda81f2b75c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "🔍 Testing model: HE_10_rounds_16k\n",
            "============================================================\n",
            "✅ Loaded model: /content/drive/MyDrive/FYP_FL_IDS/models/cnn_lstm_global_with_HE_12rounds_16k.pt\n",
            "============================================================\n",
            "Benign          → BENIGN ✅   | prob=0.1442\n",
            "DDoS            → ATTACK 🚨   | prob=0.8618\n",
            "Slow Attack     → ATTACK 🚨   | prob=0.8605\n",
            "Port Scan       → BENIGN ✅   | prob=0.1425\n",
            "Brute Force     → BENIGN ✅   | prob=0.0850\n",
            "Botnet          → ATTACK 🚨   | prob=0.7993\n",
            "Exfiltration    → BENIGN ✅   | prob=0.4321\n",
            "Hybrid          → BENIGN ✅   | prob=0.0072\n",
            "============================================================\n",
            "\n",
            "============================================================\n",
            "🔍 Testing model: 16k20 rounds\n",
            "============================================================\n",
            "✅ Loaded model: /content/drive/MyDrive/FYP_FL_IDS/models/cnn_lstm_global_with_HE_20rounds_16k.pt\n",
            "============================================================\n",
            "Benign          → BENIGN ✅   | prob=0.0951\n",
            "DDoS            → ATTACK 🚨   | prob=0.7983\n",
            "Slow Attack     → ATTACK 🚨   | prob=0.7965\n",
            "Port Scan       → BENIGN ✅   | prob=0.1303\n",
            "Brute Force     → BENIGN ✅   | prob=0.1648\n",
            "Botnet          → ATTACK 🚨   | prob=0.8854\n",
            "Exfiltration    → ATTACK 🚨   | prob=0.7894\n",
            "Hybrid          → BENIGN ✅   | prob=0.0052\n",
            "============================================================\n",
            "\n",
            "============================================================\n",
            "🔍 Testing model: HE_25_rounds_16k\n",
            "============================================================\n",
            "✅ Loaded model: /content/drive/MyDrive/FYP_FL_IDS/models/cnn_lstm_global_with_HE_25rounds_16k.pt\n",
            "============================================================\n",
            "Benign          → BENIGN ✅   | prob=0.0988\n",
            "DDoS            → ATTACK 🚨   | prob=0.8857\n",
            "Slow Attack     → ATTACK 🚨   | prob=0.8861\n",
            "Port Scan       → BENIGN ✅   | prob=0.2217\n",
            "Brute Force     → BENIGN ✅   | prob=0.2510\n",
            "Botnet          → ATTACK 🚨   | prob=0.9433\n",
            "Exfiltration    → ATTACK 🚨   | prob=0.8878\n",
            "Hybrid          → BENIGN ✅   | prob=0.0050\n",
            "============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_single_sample(sample, model, threshold=0.3):\n",
        "    \"\"\"\n",
        "    sample: Tensor or numpy array of shape (SEQ_LEN, NUM_FEATURES)\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    if isinstance(sample, np.ndarray):\n",
        "        sample = torch.tensor(sample, dtype=torch.float32)\n",
        "\n",
        "    x = sample.unsqueeze(0).to(DEVICE)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        logit = model(x)\n",
        "        prob = torch.sigmoid(logit).item()\n",
        "\n",
        "    label = \"ATTACK 🚨\" if prob > threshold else \"BENIGN ✅\"\n",
        "    return prob, label\n",
        "\n",
        "\n",
        "sample_x, sample_y = next(iter(eval_loader))\n",
        "sample_x = sample_x[0]\n",
        "sample_y = sample_y[0].item()\n",
        "\n",
        "prob, label = predict_single_sample(sample_x, model, threshold=THRESHOLD)\n",
        "\n",
        "print(\"\\n🚦 Demo Prediction\")\n",
        "print(\"=\"*30)\n",
        "print(\"Ground Truth:\", \"ATTACK\" if sample_y == 1 else \"BENIGN\")\n",
        "print(f\"Attack Probability: {prob:.4f}\")\n",
        "print(\"Prediction:\", label)\n",
        "print(\"=\"*30)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KyZfjqR3q_7o",
        "outputId": "79c609dc-96ee-4565-bb4d-97e28d27a4be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🚦 Demo Prediction\n",
            "==============================\n",
            "Ground Truth: ATTACK\n",
            "Attack Probability: 0.9888\n",
            "Prediction: ATTACK 🚨\n",
            "==============================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F1seLLxxg4hb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db99360f-286d-4f47-f425-158ebcb53a41"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total weight change: 11.202270030975342\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "init_weights = torch.load(\n",
        "    os.path.join(MODELS_DIR, \"cnn_lstm_init.pt\")\n",
        ")\n",
        "final_weights = global_model.state_dict()\n",
        "\n",
        "diff = 0.0\n",
        "for k in init_weights:\n",
        "    diff += torch.norm(init_weights[k] - final_weights[k]).item()\n",
        "\n",
        "print(\"Total weight change:\", diff)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jZm0RFGAjFiS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d802af9-9dff-470f-b941-bb17d4b14b23"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation samples available: 160000\n",
            "Evaluation samples used: 5000\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Use Bank_A as evaluation source\n",
        "EVAL_CLIENT = \"Bank_A\"\n",
        "EVAL_DIR = os.path.join(PROCESSED_DATA_DIR, \"federated_clients\", EVAL_CLIENT)\n",
        "\n",
        "eval_dataset = ClientSequenceDataset(EVAL_DIR)\n",
        "\n",
        "# Limit evaluation size (important for speed)\n",
        "EVAL_SAMPLES = 5000\n",
        "\n",
        "eval_loader = DataLoader(\n",
        "    eval_dataset,\n",
        "    batch_size=128,\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "print(\"Evaluation samples available:\", len(eval_dataset))\n",
        "print(\"Evaluation samples used:\", EVAL_SAMPLES)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MOjPzGysjI1h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0c77b35-582b-464b-b168-8973625615e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Benign indices: 1000\n",
            "Attack indices: 1000\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Load ONLY labels (fast)\n",
        "y_indices_0 = []\n",
        "y_indices_1 = []\n",
        "\n",
        "max_per_class = 1000  # reduce further if needed\n",
        "\n",
        "seen = 0\n",
        "\n",
        "for y_file in eval_dataset.y_files:\n",
        "    y_chunk = np.load(y_file, mmap_mode=\"r\")\n",
        "\n",
        "    for i, label in enumerate(y_chunk):\n",
        "        global_idx = seen + i\n",
        "\n",
        "        if label == 0 and len(y_indices_0) < max_per_class:\n",
        "            y_indices_0.append(global_idx)\n",
        "        elif label == 1 and len(y_indices_1) < max_per_class:\n",
        "            y_indices_1.append(global_idx)\n",
        "\n",
        "        if len(y_indices_0) >= max_per_class and len(y_indices_1) >= max_per_class:\n",
        "            break\n",
        "\n",
        "    seen += len(y_chunk)\n",
        "    if len(y_indices_0) >= max_per_class and len(y_indices_1) >= max_per_class:\n",
        "        break\n",
        "\n",
        "print(\"Benign indices:\", len(y_indices_0))\n",
        "print(\"Attack indices:\", len(y_indices_1))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TKUWQfOwjxaE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "017ad36a-ff84-488c-ce37-69ac74660730"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation samples used: 2000\n"
          ]
        }
      ],
      "source": [
        "global_model.eval()\n",
        "\n",
        "y_true, y_pred, y_prob = [], [], []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for idx in y_indices_0 + y_indices_1:\n",
        "        x, y = eval_dataset[idx]\n",
        "        x = x.unsqueeze(0).to(DEVICE)\n",
        "\n",
        "        prob = global_model(x).item()\n",
        "        pred = int(prob > 0.3)\n",
        "\n",
        "        y_true.append(int(y))\n",
        "        y_prob.append(prob)\n",
        "        y_pred.append(pred)\n",
        "\n",
        "print(\"Evaluation samples used:\", len(y_true))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qUeVNaonjyBU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a08be0d-ccad-4ca2-b9fb-3cc469de7e82"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy: 0.6965\n",
            "precision: 0.6661031276415892\n",
            "recall: 0.788\n",
            "f1_score: 0.7219422812643151\n",
            "roc_auc: 0.866047\n",
            "confusion_matrix: [[605, 395], [212, 788]]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score,\n",
        "    f1_score, confusion_matrix, roc_auc_score\n",
        ")\n",
        "\n",
        "metrics = {\n",
        "    \"accuracy\": accuracy_score(y_true, y_pred),\n",
        "    \"precision\": precision_score(y_true, y_pred, zero_division=0),\n",
        "    \"recall\": recall_score(y_true, y_pred, zero_division=0),\n",
        "    \"f1_score\": f1_score(y_true, y_pred, zero_division=0),\n",
        "    \"roc_auc\": roc_auc_score(y_true, y_prob),\n",
        "    \"confusion_matrix\": confusion_matrix(y_true, y_pred).tolist()\n",
        "}\n",
        "\n",
        "for k, v in metrics.items():\n",
        "    print(f\"{k}: {v}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h6FGZXYSgHp_"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "metadata = {\n",
        "    \"model_version\": MODEL_VERSION,\n",
        "    \"architecture\": \"CNN-LSTM\",\n",
        "    \"num_features\": NUM_FEATURES,\n",
        "    \"sequence_length\": SEQ_LEN,\n",
        "    \"loss_function\": \"BCEWithLogitsLoss\",\n",
        "    \"pos_weight\": float(POS_WEIGHT.cpu().numpy()),\n",
        "    \"threshold\": THRESHOLD,\n",
        "    \"rounds\": CONFIG[\"ROUNDS\"],\n",
        "    \"local_epochs\": CONFIG[\"LOCAL_EPOCHS\"],\n",
        "    \"batch_size\": CONFIG[\"BATCH_SIZE\"]\n",
        "}\n",
        "\n",
        "meta_path = os.path.join(\n",
        "    MODELS_DIR, f\"model_metadata_{MODEL_VERSION}.json\"\n",
        ")\n",
        "\n",
        "with open(meta_path, \"w\") as f:\n",
        "    json.dump(metadata, f, indent=4)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xM2NVLdcb6Iy"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_curve\n",
        "\n",
        "fpr, tpr, thresholds = roc_curve(y_true, y_scores)\n",
        "\n",
        "from sklearn.metrics import f1_score\n",
        "import numpy as np\n",
        "\n",
        "best_f1 = 0\n",
        "best_thresh = 0.5\n",
        "\n",
        "for t in np.arange(0.2, 0.8, 0.05):\n",
        "    preds = (y_scores > t).astype(int)\n",
        "    f1 = f1_score(y_true, preds)\n",
        "    if f1 > best_f1:\n",
        "        best_f1 = f1\n",
        "        best_thresh = t\n",
        "\n",
        "print(best_thresh, best_f1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        },
        "id": "Lcq1dyf3vKk9",
        "outputId": "8b739d4f-e000-4346-b36d-4e6b0eefc04f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'y_true' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4034131345.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mroc_curve\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mfpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthresholds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroc_curve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_scores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mf1_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'y_true' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "NUM_FEATURES = 78\n",
        "\n",
        "def benign_flow():\n",
        "    return np.random.normal(0.05, 0.05, NUM_FEATURES)\n",
        "\n",
        "def ddos_flow():\n",
        "    x = np.random.normal(1.2, 0.8, NUM_FEATURES)\n",
        "    x[:6] += 3.5\n",
        "    return x\n",
        "\n",
        "async def iot_device(device_id, queue):\n",
        "    attack = False\n",
        "    counter = 0\n",
        "\n",
        "    while True:\n",
        "        counter += 1\n",
        "        if counter > 25:\n",
        "            attack = True\n",
        "\n",
        "        flow = ddos_flow() if attack else benign_flow()\n",
        "\n",
        "        await queue.put({\n",
        "            \"device_id\": device_id,\n",
        "            \"features\": flow\n",
        "        })\n",
        "\n",
        "        await asyncio.sleep(random.uniform(0.5, 1.5))\n"
      ],
      "metadata": {
        "id": "tkRXb-xmsIkC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from collections import deque\n",
        "\n",
        "SEQ_LEN = 10\n",
        "THRESHOLD = 0.5\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "windows = {}\n",
        "\n",
        "async def edge_gateway(queue, model):\n",
        "    while True:\n",
        "        msg = await queue.get()\n",
        "\n",
        "        device_id = msg[\"device_id\"]\n",
        "        flow = msg[\"features\"]\n",
        "\n",
        "        if device_id not in windows:\n",
        "            windows[device_id] = deque(maxlen=SEQ_LEN)\n",
        "\n",
        "        windows[device_id].append(flow)\n",
        "\n",
        "        if len(windows[device_id]) < SEQ_LEN:\n",
        "            continue\n",
        "\n",
        "        sample = torch.tensor(\n",
        "            np.array(windows[device_id]),\n",
        "            dtype=torch.float32\n",
        "        ).unsqueeze(0).to(DEVICE)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            prob = torch.sigmoid(model(sample)).item()\n",
        "\n",
        "        decision = \"🚨 ATTACK\" if prob > THRESHOLD else \"✅ BENIGN\"\n",
        "\n",
        "        print(f\"[EDGE] Device={device_id:<8} \" f\"Window={SEQ_LEN} \" f\"Prob={prob:.4f} → {decision}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "GB7z8-g3sLlX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "  # adjust path if needed\n",
        "\n",
        "MODEL_PATH = \"/content/drive/MyDrive/FYP_FL_IDS/models/cnn_lstm_global_with_HE_25rounds_16k.pt\"\n",
        "\n",
        "model = CNN_LSTM_IDS(SEQ_LEN, NUM_FEATURES).to(DEVICE)\n",
        "model.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))\n",
        "model.eval()\n",
        "\n",
        "print(\"✅ Edge IDS model loaded\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h0T8lLwPteRd",
        "outputId": "40814596-8033-4d9f-8e02-f644b8436e13"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Edge IDS model loaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "async def main():\n",
        "    queue = asyncio.Queue()\n",
        "\n",
        "    devices = [\n",
        "        asyncio.create_task(iot_device(\"device_1\", queue)),\n",
        "        asyncio.create_task(iot_device(\"device_2\", queue)),\n",
        "        asyncio.create_task(iot_device(\"device_3\", queue)),\n",
        "    ]\n",
        "\n",
        "    edge = asyncio.create_task(edge_gateway(queue, model))\n",
        "\n",
        "    await asyncio.gather(*devices, edge)\n",
        "\n",
        "await main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "s2ycuC-9sOGQ",
        "outputId": "da933f54-e121-4e38-f4e9-92370f4fe1d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[EDGE] Device=device_1 Window=10 Prob=0.0809 → ✅ BENIGN\n",
            "[EDGE] Device=device_3 Window=10 Prob=0.0742 → ✅ BENIGN\n",
            "[EDGE] Device=device_1 Window=10 Prob=0.0731 → ✅ BENIGN\n",
            "[EDGE] Device=device_2 Window=10 Prob=0.0908 → ✅ BENIGN\n",
            "[EDGE] Device=device_3 Window=10 Prob=0.0881 → ✅ BENIGN\n",
            "[EDGE] Device=device_2 Window=10 Prob=0.0864 → ✅ BENIGN\n",
            "[EDGE] Device=device_1 Window=10 Prob=0.0864 → ✅ BENIGN\n",
            "[EDGE] Device=device_1 Window=10 Prob=0.0836 → ✅ BENIGN\n",
            "[EDGE] Device=device_3 Window=10 Prob=0.0812 → ✅ BENIGN\n",
            "[EDGE] Device=device_2 Window=10 Prob=0.0788 → ✅ BENIGN\n",
            "[EDGE] Device=device_3 Window=10 Prob=0.0806 → ✅ BENIGN\n",
            "[EDGE] Device=device_1 Window=10 Prob=0.0812 → ✅ BENIGN\n",
            "[EDGE] Device=device_2 Window=10 Prob=0.0736 → ✅ BENIGN\n",
            "[EDGE] Device=device_2 Window=10 Prob=0.0803 → ✅ BENIGN\n",
            "[EDGE] Device=device_3 Window=10 Prob=0.0785 → ✅ BENIGN\n",
            "[EDGE] Device=device_1 Window=10 Prob=0.0861 → ✅ BENIGN\n",
            "[EDGE] Device=device_2 Window=10 Prob=0.0794 → ✅ BENIGN\n",
            "[EDGE] Device=device_3 Window=10 Prob=0.0787 → ✅ BENIGN\n",
            "[EDGE] Device=device_1 Window=10 Prob=0.0843 → ✅ BENIGN\n",
            "[EDGE] Device=device_2 Window=10 Prob=0.0859 → ✅ BENIGN\n",
            "[EDGE] Device=device_3 Window=10 Prob=0.0912 → ✅ BENIGN\n",
            "[EDGE] Device=device_2 Window=10 Prob=0.0872 → ✅ BENIGN\n",
            "[EDGE] Device=device_1 Window=10 Prob=0.0985 → ✅ BENIGN\n",
            "[EDGE] Device=device_2 Window=10 Prob=0.0831 → ✅ BENIGN\n",
            "[EDGE] Device=device_3 Window=10 Prob=0.0920 → ✅ BENIGN\n",
            "[EDGE] Device=device_1 Window=10 Prob=0.0930 → ✅ BENIGN\n",
            "[EDGE] Device=device_2 Window=10 Prob=0.0664 → ✅ BENIGN\n",
            "[EDGE] Device=device_3 Window=10 Prob=0.0905 → ✅ BENIGN\n",
            "[EDGE] Device=device_1 Window=10 Prob=0.0787 → ✅ BENIGN\n",
            "[EDGE] Device=device_3 Window=10 Prob=0.0824 → ✅ BENIGN\n",
            "[EDGE] Device=device_1 Window=10 Prob=0.0830 → ✅ BENIGN\n",
            "[EDGE] Device=device_2 Window=10 Prob=0.0749 → ✅ BENIGN\n",
            "[EDGE] Device=device_1 Window=10 Prob=0.0850 → ✅ BENIGN\n",
            "[EDGE] Device=device_2 Window=10 Prob=0.0822 → ✅ BENIGN\n",
            "[EDGE] Device=device_3 Window=10 Prob=0.0910 → ✅ BENIGN\n",
            "[EDGE] Device=device_2 Window=10 Prob=0.0812 → ✅ BENIGN\n",
            "[EDGE] Device=device_1 Window=10 Prob=0.0882 → ✅ BENIGN\n",
            "[EDGE] Device=device_3 Window=10 Prob=0.0828 → ✅ BENIGN\n",
            "[EDGE] Device=device_2 Window=10 Prob=0.0797 → ✅ BENIGN\n",
            "[EDGE] Device=device_3 Window=10 Prob=0.1048 → ✅ BENIGN\n",
            "[EDGE] Device=device_3 Window=10 Prob=0.0866 → ✅ BENIGN\n",
            "[EDGE] Device=device_1 Window=10 Prob=0.0914 → ✅ BENIGN\n",
            "[EDGE] Device=device_3 Window=10 Prob=0.0840 → ✅ BENIGN\n",
            "[EDGE] Device=device_2 Window=10 Prob=0.0940 → ✅ BENIGN\n",
            "[EDGE] Device=device_1 Window=10 Prob=0.0671 → ✅ BENIGN\n",
            "[EDGE] Device=device_2 Window=10 Prob=0.0781 → ✅ BENIGN\n",
            "[EDGE] Device=device_1 Window=10 Prob=0.0660 → ✅ BENIGN\n",
            "[EDGE] Device=device_3 Window=10 Prob=0.0853 → ✅ BENIGN\n",
            "[EDGE] Device=device_1 Window=10 Prob=0.0129 → ✅ BENIGN\n",
            "[EDGE] Device=device_2 Window=10 Prob=0.0011 → ✅ BENIGN\n",
            "[EDGE] Device=device_3 Window=10 Prob=0.0100 → ✅ BENIGN\n",
            "[EDGE] Device=device_1 Window=10 Prob=0.0028 → ✅ BENIGN\n",
            "[EDGE] Device=device_2 Window=10 Prob=0.0043 → ✅ BENIGN\n",
            "[EDGE] Device=device_3 Window=10 Prob=0.1305 → ✅ BENIGN\n",
            "[EDGE] Device=device_1 Window=10 Prob=0.0186 → ✅ BENIGN\n",
            "[EDGE] Device=device_3 Window=10 Prob=0.0481 → ✅ BENIGN\n",
            "[EDGE] Device=device_2 Window=10 Prob=0.0155 → ✅ BENIGN\n",
            "[EDGE] Device=device_3 Window=10 Prob=0.0188 → ✅ BENIGN\n",
            "[EDGE] Device=device_1 Window=10 Prob=0.1315 → ✅ BENIGN\n",
            "[EDGE] Device=device_2 Window=10 Prob=0.0031 → ✅ BENIGN\n",
            "[EDGE] Device=device_3 Window=10 Prob=0.0068 → ✅ BENIGN\n",
            "[EDGE] Device=device_1 Window=10 Prob=0.0092 → ✅ BENIGN\n",
            "[EDGE] Device=device_2 Window=10 Prob=0.0962 → ✅ BENIGN\n",
            "[EDGE] Device=device_1 Window=10 Prob=0.1348 → ✅ BENIGN\n",
            "[EDGE] Device=device_2 Window=10 Prob=0.0194 → ✅ BENIGN\n",
            "[EDGE] Device=device_3 Window=10 Prob=0.0090 → ✅ BENIGN\n",
            "[EDGE] Device=device_3 Window=10 Prob=0.1642 → ✅ BENIGN\n",
            "[EDGE] Device=device_1 Window=10 Prob=0.1911 → ✅ BENIGN\n",
            "[EDGE] Device=device_3 Window=10 Prob=0.0706 → ✅ BENIGN\n",
            "[EDGE] Device=device_2 Window=10 Prob=0.2048 → ✅ BENIGN\n",
            "[EDGE] Device=device_1 Window=10 Prob=0.0346 → ✅ BENIGN\n",
            "[EDGE] Device=device_3 Window=10 Prob=0.0101 → ✅ BENIGN\n",
            "[EDGE] Device=device_2 Window=10 Prob=0.5002 → 🚨 ATTACK\n",
            "[EDGE] Device=device_1 Window=10 Prob=0.3538 → ✅ BENIGN\n",
            "[EDGE] Device=device_2 Window=10 Prob=0.2854 → ✅ BENIGN\n",
            "[EDGE] Device=device_3 Window=10 Prob=0.0591 → ✅ BENIGN\n",
            "[EDGE] Device=device_1 Window=10 Prob=0.6641 → 🚨 ATTACK\n",
            "[EDGE] Device=device_2 Window=10 Prob=0.7095 → 🚨 ATTACK\n",
            "[EDGE] Device=device_3 Window=10 Prob=0.0797 → ✅ BENIGN\n",
            "[EDGE] Device=device_3 Window=10 Prob=0.1806 → ✅ BENIGN\n",
            "[EDGE] Device=device_2 Window=10 Prob=0.2306 → ✅ BENIGN\n",
            "[EDGE] Device=device_1 Window=10 Prob=0.0284 → ✅ BENIGN\n",
            "[EDGE] Device=device_3 Window=10 Prob=0.2070 → ✅ BENIGN\n",
            "[EDGE] Device=device_2 Window=10 Prob=0.6148 → 🚨 ATTACK\n",
            "[EDGE] Device=device_1 Window=10 Prob=0.2231 → ✅ BENIGN\n",
            "[EDGE] Device=device_2 Window=10 Prob=0.1148 → ✅ BENIGN\n",
            "[EDGE] Device=device_1 Window=10 Prob=0.2768 → ✅ BENIGN\n",
            "[EDGE] Device=device_2 Window=10 Prob=0.1730 → ✅ BENIGN\n",
            "[EDGE] Device=device_3 Window=10 Prob=0.3208 → ✅ BENIGN\n",
            "[EDGE] Device=device_2 Window=10 Prob=0.3816 → ✅ BENIGN\n",
            "[EDGE] Device=device_1 Window=10 Prob=0.1027 → ✅ BENIGN\n",
            "[EDGE] Device=device_3 Window=10 Prob=0.6390 → 🚨 ATTACK\n",
            "[EDGE] Device=device_1 Window=10 Prob=0.0972 → ✅ BENIGN\n",
            "[EDGE] Device=device_2 Window=10 Prob=0.3530 → ✅ BENIGN\n",
            "[EDGE] Device=device_1 Window=10 Prob=0.8428 → 🚨 ATTACK\n",
            "[EDGE] Device=device_3 Window=10 Prob=0.4827 → ✅ BENIGN\n",
            "[EDGE] Device=device_2 Window=10 Prob=0.0966 → ✅ BENIGN\n",
            "[EDGE] Device=device_2 Window=10 Prob=0.1362 → ✅ BENIGN\n",
            "[EDGE] Device=device_1 Window=10 Prob=0.6202 → 🚨 ATTACK\n",
            "[EDGE] Device=device_3 Window=10 Prob=0.0680 → ✅ BENIGN\n",
            "[EDGE] Device=device_3 Window=10 Prob=0.4157 → ✅ BENIGN\n",
            "[EDGE] Device=device_1 Window=10 Prob=0.7502 → 🚨 ATTACK\n",
            "[EDGE] Device=device_2 Window=10 Prob=0.5910 → 🚨 ATTACK\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "CancelledError",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mCancelledError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3843380586.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mawait\u001b[0m \u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mdevices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mawait\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-3843380586.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0medge\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_task\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0medge_gateway\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0;32mawait\u001b[0m \u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mdevices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1333551803.py\u001b[0m in \u001b[0;36miot_device\u001b[0;34m(device_id, queue)\u001b[0m\n\u001b[1;32m     29\u001b[0m         })\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0;32mawait\u001b[0m \u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muniform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/lib/python3.12/asyncio/tasks.py\u001b[0m in \u001b[0;36msleep\u001b[0;34m(delay, result)\u001b[0m\n\u001b[1;32m    663\u001b[0m                         future, result)\n\u001b[1;32m    664\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 665\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    666\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    667\u001b[0m         \u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcancel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mCancelledError\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "\n",
        "# =========================\n",
        "# CONFIG\n",
        "# =========================\n",
        "MODEL_PATH = \"models/cnn_lstm_global_with_HE_25rounds_16k.pt\"\n",
        "SEQ_LEN = 10\n",
        "NUM_FEATURES = 78\n",
        "THRESHOLD = 0.5\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# =========================\n",
        "# LOAD MODEL\n",
        "# =========================\n",
        "import CNN_LSTM_IDS   # adjust import if needed\n",
        "\n",
        "model = CNN_LSTM_IDS(SEQ_LEN, NUM_FEATURES).to(DEVICE)\n",
        "model.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))\n",
        "model.eval()\n",
        "\n",
        "print(\"✅ Edge IDS Model Loaded\")\n",
        "\n",
        "# =========================\n",
        "# TRAFFIC GENERATORS\n",
        "# =========================\n",
        "def benign_flow():\n",
        "    return np.random.normal(0.05, 0.05, NUM_FEATURES)\n",
        "\n",
        "def ddos_flow():\n",
        "    x = np.random.normal(1.2, 0.8, NUM_FEATURES)\n",
        "    x[:6] += 3.5\n",
        "    return x\n",
        "\n",
        "def slow_attack_flow():\n",
        "    x = np.random.normal(0.6, 0.3, NUM_FEATURES)\n",
        "    x[20:25] += 1.5\n",
        "    return x\n",
        "\n",
        "# =========================\n",
        "# EDGE IDS LOGIC\n",
        "# =========================\n",
        "window = deque(maxlen=SEQ_LEN)\n",
        "\n",
        "def edge_process(flow):\n",
        "    window.append(flow)\n",
        "\n",
        "    if len(window) < SEQ_LEN:\n",
        "        return None\n",
        "\n",
        "    sample = torch.tensor(np.array(window), dtype=torch.float32)\n",
        "    sample = sample.unsqueeze(0).to(DEVICE)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        prob = torch.sigmoid(model(sample)).item()\n",
        "\n",
        "    decision = \"ATTACK 🚨\" if prob > THRESHOLD else \"BENIGN ✅\"\n",
        "    return prob, decision\n",
        "\n",
        "# =========================\n",
        "# DEMO RUN\n",
        "# =========================\n",
        "print(\"\\n=== EDGE IDS DEMO START ===\\n\")\n",
        "\n",
        "traffic_sequence = (\n",
        "    [\"BENIGN\"] * 10 +\n",
        "    [\"DDoS\"] * 10 +\n",
        "    [\"SLOW_ATTACK\"] * 10\n",
        ")\n",
        "\n",
        "for t, traffic_type in enumerate(traffic_sequence):\n",
        "    if traffic_type == \"BENIGN\":\n",
        "        flow = benign_flow()\n",
        "    elif traffic_type == \"DDoS\":\n",
        "        flow = ddos_flow()\n",
        "    else:\n",
        "        flow = slow_attack_flow()\n",
        "\n",
        "    result = edge_process(flow)\n",
        "\n",
        "    if result:\n",
        "        prob, decision = result\n",
        "        print(f\"[EDGE] Traffic={traffic_type:<12} \"\n",
        "              f\"Prob={prob:.4f} → {decision}\")\n",
        "\n",
        "print(\"\\n=== EDGE IDS DEMO END ===\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 395
        },
        "id": "mYMt3ofjs9oq",
        "outputId": "e0479cc5-7f56-4cb5-e389-fab5a39746ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'CNN_LSTM_IDS'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2263975364.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# LOAD MODEL\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# =========================\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mCNN_LSTM_IDS\u001b[0m   \u001b[0;31m# adjust import if needed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCNN_LSTM_IDS\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSEQ_LEN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNUM_FEATURES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'CNN_LSTM_IDS'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}